{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380d250f-20f9-4adc-a773-5bf479a27e5d",
   "metadata": {},
   "source": [
    "## Language Modelling\n",
    "Language Modelling is modelling the probability of a sequence: \n",
    "$$\n",
    "P(w_1, w_2, ..., w_n)=P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2)...P(w_n|w_1,w_2,...,w_{n-1})=\\prod_{t=1}^{T}P(w_t|w_1,w_2,...,w_{t-1})\n",
    "$$\n",
    "An early statistical language model is the **n-gram model**. It assumes that the nth word is dependent only on the preceding n-1 words to make probability computation feasible. For example, a bi-gram model use one preceding word to predict the next word, a tri-gram model uses two preceding words to predict the next word...\n",
    "\n",
    "But with n-gram model, we generate grammartical but incoherent texts that don't make sense. So let's see neural language models.\n",
    "\n",
    "## How to Build Neural Language Models(NLM)\n",
    "### A fixed-window NLM\n",
    "It follows the convention of n-gram---predict the nth word given preceding n-1 words. Word vectors are concatenated into a window vector. And it receives the window vector, processes it with an MLP and outputs softmax probabilities over the entire vocabulary. However, one big problem of this model is that it cannot handle variable sequence length dependency. So we'll see RNN.\n",
    "\n",
    "### RNN\n",
    "#### Model Structure\n",
    "RNN is like a reusable MLP. An RNN receives two inputs every step. We input a vector $x_1$ (it can be the first word vector of a sentence), and initialise a hidden state $h_0$ (usually a zero vector), take their respective affine transformations $W_{xh}x_1$ and $W_{hh}h_0$, and then plus a bias $b_h$, sum them up, we get $W_{xh}x_1+W_{hh}h_0+b_h$. And then we apply an activation function $\\Phi$, we get $\\Phi(W_{xh}x_1+W_{hh}h_0+b_h)$. This is what we use to compute the current hidden state $h_1$. To get an output of the current time step, just do an affine transformation with $h_1$ and add a bias $b_o$, apply an activation function. We can do this calculation recurrently: input $x_2$ (the second word vector of a sentence), load $h_1$...... This is how an RNN works. The main maths are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_t=\\Phi(W_{xh}x_t+W_{hh}h_{t-1}+b_h) \\\\\n",
    "o_t=\\phi(W_{ho}h_t+b_o)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "I also have a diagram:\n",
    "<center>\n",
    "<img src=\"./RNN.png\" width=500 height=500>\n",
    "</center>\n",
    "\n",
    "#### Training an RNN\n",
    "\n",
    "Let's say we are doing a Next-Word-Prediction task and we have a huge corpus starting with \"the students opened their exams\". When we input \"the\", we get the prob of the next word. However, we don't choose the most probable word as the next input. Instead, we input the ground truth \"students\". This is called \"teacher forcing\", aiming to avoid error accumulation, et cetera. And then input \"opened\"... Every prediction, we compute a loss. In practice, we don't just run through the entire corpus and compute all losses. It's too computationally expensive. Rather, we split the corpus into sentences( or documents), put them into batches. In a batch of sentences, losses of words of different sentences are computed in parallel. To do backprob, we compute the average losses of every sentence i.e., $\\frac{loss_{w1}+loss_{w2}+...}{num\\_words}$, then average the sentence-level losses, i,e., $\\frac{loss_{s1}+loss_{s2}+...}{batch\\_size}$, finally do mini batch gradient descent. Well, this is equivalent to averaging the losses of all tokens in a batch, which is what we in practice do.<a id=\"one\"></a>\n",
    "\n",
    "##### Gradient Calculation:\n",
    "The gradient of loss at time step $t$ w.r.t. recurrent weights is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J^{(t)}}{\\partial W_{hh}}\n",
    "&= \\sum_{i=1}^{t}{\\frac{\\partial J^{(t)}}{\\partial W_{hh}}} \\Big|_{(i)} \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{hh}} \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot (\\frac{\\partial h_t}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial W_{hh}}) \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot (\\frac{\\partial h_t}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot (\\frac{\\partial h_{t-1}}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdot \\frac{\\partial h_{t-2}}{\\partial W_{hh}})) \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot (\\frac{\\partial h_t}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot (\\frac{\\partial h_{t-1}}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdot (\\frac{\\partial h_{t-2}}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_{t-2}}{\\partial h_{t-3}} \\cdot \\frac{\\partial h_{t-3}}{\\partial W_{hh}}))) \\\\\n",
    "&= \\text{...} \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial W_{hh}} \\Big|_{dir} + ... + \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdot ... \\cdot \\frac{\\partial h_{2}}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial W_{hh}} \\Big|_{dir}\n",
    "\\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "**N.B: The sign \"$\\Big|_{dir}$\" denotes the direct partial derivative. When calculating direct partial derivatives, treat intermediate variables as constants.**\n",
    "\n",
    "To get the gradient of the loss over the entire sequence, sum the gradients contributed by all time steps:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W_{hh}}=\\sum^{T}_{t=1}{\\frac{\\partial J^{(t)}}{\\partial W_{hh}}}\n",
    "$$\n",
    "\n",
    "As for the gradient of loss at time step $t$ w.r.t. **input weights**, the form is the same as above, just replace $W_{hh}$ with $W_{xh}$. And the gradient of loss w.r.t. **output weights**, they don't depend on time steps thus easy to calculate.\n",
    "\n",
    "##### Gradient Clipping\n",
    "As you can see from the formula (1), the length of matrix mulplication chain is O(t), which is prone to causing gradient instability problems. For gradient vanishing, there's no general solution. But for gradient exploding, we have **Gradient Clipping**. The main idea is: Put all gradients into a long one dimensional vector $g$, calculate its L2 norm, which reflects the magnitude of the overall gradient. If it's too large, clip it.\n",
    "\n",
    "More specifically, we have gradient matrices: $\\partial W_{xh}$, $\\partial W_{hh}$ and $\\partial W_{ho}$ (biases not considered for simplification). We take all gradient values to form a vector $g$ in the shape of (x*h+h*h+h*o), calculate its L2 norm $||g||$. Next we compare this value with a threshold $\\theta$. $||g||$ being larger than $\\theta$ means that gradients need to be clipped(scaled). We scale $g$ by $\\frac{\\theta}{||g||}$. If not, we do nothing or just consider $\\theta$ to be 1.\n",
    "\n",
    "$$\n",
    "g‚Üêmin(1, \\frac{\\theta}{||g||}) \\cdot g\n",
    "$$\n",
    "\n",
    "Doing so, we scale down exploding gradients to relatively moderate values. So $g$ are the gradients with which we do weight updates.\n",
    "\n",
    "#### Application: Generating Texts\n",
    "In put the first word, get the probabilities of the next word, sample one word as the input for the next step... Recurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8f15ae5f-7e19-4886-95dc-359197b36246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN implementation from scratch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "def get_params(num_inputs, num_hiddens, num_outputs): # the size of input layer, hidden layer and output layer.\n",
    "\n",
    "    def norm(shape):\n",
    "        return torch.randn(size=shape) * 0.01\n",
    "\n",
    "    W_xh=norm((num_inputs, num_hiddens))\n",
    "    W_hh=norm((num_hiddens, num_hiddens))\n",
    "    W_ho=norm((num_hiddens, num_outputs))\n",
    "    b_h=torch.zeros((num_hiddens))\n",
    "    b_o=torch.zeros((num_outputs))\n",
    "    params=[W_xh, W_hh, b_h, W_ho, b_o]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "def init_rnn_hidden_state(batch_size, num_hiddens):\n",
    "    return torch.zeros((batch_size, num_hiddens))\n",
    "\n",
    "def rnn(inputs, init_state, params):\n",
    "    W_xh, W_hh, b_h, W_ho, b_o=params\n",
    "    H=init_state #shape: (batch_size, num_hiddens)\n",
    "    outputs=[]\n",
    "    for X in inputs:# shape of inputs: (sequence_length, batch_size, num_inputs/input_dim)\n",
    "        H=torch.tanh(X@W_xh + H@W_hh + b_h)\n",
    "        O=H@W_ho + b_o #shape:(batch_size, num_outputs)\n",
    "        outputs.append(O) # list of (batch_size, num_outputs) with a length of \"sequence_length\"\n",
    "    return (torch.concat(outputs, dim=0), #shape: (sequence_length*batch_size, num_outputs)\n",
    "           H) #last_hidden_state\n",
    "\n",
    "class RNNFromScratch():\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, get_params, init_rnn_hidden_state, forward_fn):\n",
    "        self.num_inputs, self.num_hiddens, self.num_outputs=num_inputs, num_hiddens, num_outputs\n",
    "        self.params=get_params(self.num_inputs, self.num_hiddens, self.num_outputs)\n",
    "        self.init_rnn_hidden_state, self.forward_fn=init_rnn_hidden_state, forward_fn\n",
    "\n",
    "    def __call__(self, inputs, init_state):\n",
    "        #shape of inputs: (sequence_length, batch_size, num_inputs), so you need to do the encoding yourself.\n",
    "        return self.forward_fn(inputs, init_state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size):\n",
    "        #Here I choose to relinquish the right to decide when to init RNN state.\n",
    "        return self.init_rnn_hidden_state(batch_size, self.num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ecf6a940-782a-40be-9640-94f6ef58bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I'm gonna show you how the model does a forwarding.\n",
    "I'll generate a three-word(sequence_length=3) four-sentence(batch_size=4) corpus,\n",
    "where each word is embedded into ten-dimensional(vocab_size=10).\n",
    "And the model suits the data, it has an num_input of 10. And the num_hiddens of 20 is randomly set. It outputs 10-dimensional vectors.\n",
    "'''\n",
    "X=torch.randn((3,4,10))\n",
    "model=RNNFromScratch(10, 20, 10, get_params, init_rnn_hidden_state, rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "41ac21ef-d6f7-4956-8421-41ae656acd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state=model.begin_state(4) #4=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c102e44f-973d-45f0-926e-6443335dbaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 10]),\n",
       " torch.Size([4, 20]),\n",
       " tensor([[ 7.8869e-04, -1.1282e-04, -2.2380e-03, -7.6417e-04, -1.2836e-03,\n",
       "          -4.9329e-05,  1.7520e-04,  2.7867e-04, -2.9061e-04,  1.2229e-03],\n",
       "         [-5.2602e-04, -1.0642e-03, -3.4827e-04, -1.9850e-03,  1.2060e-04,\n",
       "           3.0027e-04,  8.1262e-04,  9.8413e-04,  6.4373e-05,  9.4788e-04],\n",
       "         [ 3.0684e-04,  6.3645e-04,  2.8042e-03,  1.4256e-03,  9.6874e-04,\n",
       "           1.3954e-04, -9.9776e-04, -4.9223e-04, -5.1182e-04, -2.6976e-03],\n",
       "         [ 1.2678e-03,  3.9407e-04,  2.2582e-03,  3.6788e-04,  8.3618e-04,\n",
       "           1.5151e-03, -8.5991e-05,  1.1036e-04, -1.3444e-03, -2.1440e-03],\n",
       "         [-6.8979e-04, -4.4372e-04, -1.1161e-04, -5.0272e-04, -1.7772e-05,\n",
       "          -3.6689e-04,  1.1432e-04,  1.0340e-04,  3.2620e-04,  3.3911e-04],\n",
       "         [ 2.8142e-04, -1.3099e-05,  1.1177e-03, -7.1878e-04, -5.2349e-04,\n",
       "          -6.0484e-05, -8.3074e-04,  1.8374e-04, -1.1330e-03, -2.1839e-03],\n",
       "         [-4.2062e-04, -1.0863e-03,  1.7017e-04, -9.5346e-04, -3.9553e-04,\n",
       "           6.1607e-04, -1.0749e-03,  2.7541e-04, -3.4573e-04,  3.4445e-04],\n",
       "         [-9.6978e-04,  1.3056e-03,  3.0367e-03,  1.4388e-03,  9.1038e-04,\n",
       "           1.2041e-03,  1.5533e-04, -1.3086e-03, -1.3700e-04, -1.9175e-03],\n",
       "         [ 4.2622e-04,  1.4355e-03,  4.5595e-04, -2.2353e-03,  7.8341e-04,\n",
       "           1.8524e-03,  2.7513e-03,  7.8001e-04, -6.6767e-04, -7.9748e-04],\n",
       "         [ 9.5366e-04, -8.2165e-04, -3.6993e-03, -1.2099e-03,  7.0008e-04,\n",
       "          -9.5684e-04,  8.6819e-04,  1.5541e-03,  8.6658e-04,  2.9554e-03],\n",
       "         [ 1.0653e-03, -5.0132e-04, -2.5051e-03, -3.7023e-03,  1.6340e-05,\n",
       "           1.0089e-03,  1.6205e-03,  1.8393e-03,  5.6382e-04,  2.5951e-03],\n",
       "         [ 4.4313e-04,  1.8262e-04,  1.7159e-03, -6.5130e-04,  4.8830e-04,\n",
       "           2.2418e-03,  5.5451e-04,  7.3632e-04, -1.0509e-03, -3.7149e-04]],\n",
       "        grad_fn=<CatBackward0>))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred, last_hidden_state=model(X, state)\n",
    "Y_pred.shape, last_hidden_state.shape, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5e4f07f4-d733-4fad-9bb2-3a04ca24ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping(model, theta):\n",
    "    if isinstance(model, nn.Module):\n",
    "        params=[p for p in model.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params=[p for p in model.params]\n",
    "    norm=torch.sqrt(sum(torch.sum(p.grad**2) for p in params))# Here we don't fisrt concat then calculate. Instead, we calculate then sum.\n",
    "    if norm>theta:\n",
    "        for param in params:\n",
    "            param*=theta/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "698b8180-a9e1-452a-916e-926a9c2b0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "Y=torch.randint(0,10,(12,))\n",
    "optimizer=torch.optim.Adam(model.params)\n",
    "criterion=torch.nn.CrossEntropyLoss()#reduction=\"mean\" by default. So we are summing the loss of every vector from every sentence(batch)\n",
    "#and average it.(just like what I said in the 1st paragraph under \"Trainging an RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "65d86d24-61a3-4875-a9e9-31039f9f4a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0171, -0.0121,  0.0270,  0.0039,  0.0031,  0.0168,  0.0184, -0.0138,\n",
       "          0.0082, -0.0005, -0.0057,  0.0111,  0.0160, -0.0196,  0.0133,  0.0086,\n",
       "         -0.0033,  0.0027,  0.0202,  0.0275],\n",
       "        [-0.0183, -0.0181, -0.0095,  0.0080, -0.0014, -0.0009,  0.0021, -0.0153,\n",
       "         -0.0129, -0.0038,  0.0044,  0.0024, -0.0106,  0.0264,  0.0148, -0.0122,\n",
       "          0.0131,  0.0002, -0.0184, -0.0232],\n",
       "        [ 0.0140,  0.0095,  0.0214,  0.0123, -0.0119, -0.0193, -0.0165,  0.0107,\n",
       "          0.0231, -0.0167,  0.0167, -0.0113, -0.0156, -0.0177, -0.0218,  0.0280,\n",
       "         -0.0255,  0.0119, -0.0181, -0.0110],\n",
       "        [ 0.0121,  0.0058,  0.0023,  0.0049, -0.0258,  0.0270,  0.0199,  0.0027,\n",
       "          0.0091, -0.0104, -0.0263,  0.0054, -0.0132, -0.0311,  0.0082,  0.0060,\n",
       "          0.0217, -0.0139,  0.0095,  0.0126],\n",
       "        [-0.0133, -0.0134, -0.0168,  0.0189,  0.0361,  0.0021, -0.0057,  0.0227,\n",
       "         -0.0199,  0.0038,  0.0160,  0.0069, -0.0195,  0.0192,  0.0024, -0.0181,\n",
       "          0.0080, -0.0112, -0.0006, -0.0131],\n",
       "        [-0.0059, -0.0212,  0.0050,  0.0223, -0.0077, -0.0103, -0.0085,  0.0264,\n",
       "          0.0128, -0.0099, -0.0170, -0.0069,  0.0169, -0.0055,  0.0080,  0.0019,\n",
       "         -0.0058,  0.0153,  0.0071,  0.0260],\n",
       "        [-0.0071,  0.0017,  0.0244, -0.0268, -0.0227, -0.0159, -0.0190,  0.0090,\n",
       "          0.0107, -0.0101,  0.0064,  0.0206,  0.0034, -0.0038, -0.0068, -0.0052,\n",
       "          0.0122,  0.0072,  0.0135,  0.0321],\n",
       "        [ 0.0065,  0.0132, -0.0320, -0.0161,  0.0008,  0.0244,  0.0121, -0.0312,\n",
       "         -0.0180,  0.0014, -0.0242, -0.0017, -0.0169, -0.0011, -0.0144,  0.0207,\n",
       "          0.0002, -0.0053, -0.0016,  0.0015],\n",
       "        [-0.0060,  0.0141,  0.0011, -0.0079, -0.0091,  0.0283,  0.0004, -0.0044,\n",
       "          0.0032,  0.0194,  0.0150,  0.0055, -0.0019, -0.0054,  0.0211, -0.0066,\n",
       "          0.0369, -0.0105,  0.0066,  0.0132],\n",
       "        [ 0.0041, -0.0129, -0.0172,  0.0307,  0.0189, -0.0104, -0.0137,  0.0352,\n",
       "         -0.0057,  0.0024,  0.0222, -0.0275, -0.0195,  0.0280, -0.0259, -0.0171,\n",
       "          0.0127, -0.0124, -0.0260, -0.0179]], requires_grad=True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred, last_hidden_state=model(X, state)\n",
    "optimizer.zero_grad()\n",
    "loss=criterion(Y_pred,Y)\n",
    "loss.backward()\n",
    "gradient_clipping(model, 1.0)\n",
    "optimizer.step()\n",
    "model.params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3a9729f0-66bc-4d06-b17c-33f6465f3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN implementation with torch api\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim , num_hiddens):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn=nn.RNN(embedding_dim, num_hiddens, batch_first=True) #num_hidden_layer is 1 by default. \n",
    "        #And h0 is by default torch.zeros((num_hiden_layers, batch_size, num_hiddens))\n",
    "        self.fc=nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def forward(self, X): # X in the shape of (batch_size, seq_length)\n",
    "        embeddings=self.embedding(X) #shape: (batch_size, seq_length, embedding_dim)\n",
    "        hidden_states, h_n=self.rnn(embeddings)\n",
    "        logits=self.fc(hidden_states.reshape(-1, hidden_states.size(-1)))\n",
    "        logits=logits.reshape((hidden_states.size(0), hidden_states.size(1), -1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2ebcf874-9e72-49a9-9b1c-e372a388d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''She Was More Like A Beauty Queen From A Movie Scene.I Said Don't Mind, But What Do You Mean I Am The One\n",
    "Who Will Dance On The Floor In The Round.She Said I Am The One Who Will Dance On The Floor In The Round.She Told Me Her Name Was Billie Jean, As She Caused A Scene\n",
    "Then Every Head Turned With Eyes That Dreamed Of Being The One.Who Will Dance On The Floor In The Round.People Always Told Me Be Careful Of What You Do\n",
    "And Don't Go Around Breaking Young Girls' Hearts.\n",
    "And Mother Always Told Me Be Careful Of Who You Love.\n",
    "And Be Careful Of What You Do 'Cause The Lie Becomes The Truth.\n",
    "Billie Jean Is Not My Lover.\n",
    "She's Just A Girl Who Claims That I Am The One.\n",
    "But The Kid Is Not My Son.\n",
    "She Says I Am The One, But The Kid Is Not My Son.\n",
    "For Forty Days And Forty Nights.\n",
    "The Law was on her Side.\n",
    "But Who Can Stand When She's In Demand.\n",
    "Her Schemes And Plans.\n",
    "'Cause We Danced On The Floor In The Round.\n",
    "So Take My Strong Advice, Just Remember To Always Think Twice.\n",
    "Do think Twice.\n",
    "She Told My Baby We had Danced unTill Three.\n",
    "Then She Looked At Me.\n",
    "She Showed A Photo Of A Baby Crying.\n",
    "His Eyes Looked Like Mine.\n",
    "Go On Dance On The Floor In The Round, Baby.\n",
    "People Always Told Me Be Careful Of What You Do.\n",
    "And Don't Go Around Breaking Young Girls' Hearts.\n",
    "She Came And Stood Right By Me.\n",
    "Then The Smell Of Sweet Perfume.\n",
    "This Happened Much Too Soon.\n",
    "She Called Me To Her Room.\n",
    "Billie Jean Is Not My Lover.\n",
    "She's Just A Girl Who Claims That I Am The One.\n",
    "But The Kid Is Not My Son.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d6a4ed3f-4739-4527-9b9f-fae8d8780f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus=corpus.lower().replace(\"\\n\", \"\")\n",
    "        self.corpus=re.sub(r\"[^\\w\\s]\",\"\",self.corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cab41-0afe-4074-87ca-5277e79b7e51",
   "metadata": {},
   "source": [
    "### Evaluation of Language Models\n",
    "#### Perplexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
