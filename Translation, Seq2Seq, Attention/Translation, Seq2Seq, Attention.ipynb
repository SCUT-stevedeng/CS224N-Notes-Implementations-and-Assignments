{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c2982c-8557-4db1-9364-155dac9d4e91",
   "metadata": {},
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0370a1eb-983b-404a-9fe7-ceaf21473842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1777a74d-e7d2-47eb-88b9-f30894226d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=torch.arange(2000).reshape(100,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f6266fc9-aa2d-4f3d-ad38-daae84e05f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 20])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4c134aad-ae10-4cc6-8cb6-bbd18c4b5383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "A=\"c\"\n",
    "match A:\n",
    "    case \"c\":\n",
    "        print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62e7a499-ced7-4910-80f8-db84a3711bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 49,  2, 36, 91, 77, 12, 86, 28, 87, 79, 60, 86, 22, 44,  2, 17, 53,\n",
       "        22, 32, 79, 94, 57, 24, 19, 90, 99, 13, 28, 28])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=idx.flatten()\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b7674bb4-7445-4c6a-8a86-ff02038fb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "73128849-3c71-473a-86a5-0d82768cf1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "torch.optim.Adam(\n",
       "    params: Union[collections.abc.Iterable[torch.Tensor], collections.abc.Iterable[dict[str, Any]], collections.abc.Iterable[tuple[str, torch.Tensor]]],\n",
       "    lr: Union[float, torch.Tensor] = \u001b[32m0.001\u001b[39m,\n",
       "    betas: tuple[typing.Union[float, torch.Tensor], typing.Union[float, torch.Tensor]] = (\u001b[32m0.9\u001b[39m, \u001b[32m0.999\u001b[39m),\n",
       "    eps: float = \u001b[32m1e-08\u001b[39m,\n",
       "    weight_decay: float = \u001b[32m0\u001b[39m,\n",
       "    amsgrad: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    *,\n",
       "    foreach: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    maximize: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    capturable: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    differentiable: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    fused: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    decoupled_weight_decay: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "Implements Adam algorithm.\n",
       "\n",
       ".. math::\n",
       "   \\begin{aligned}\n",
       "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
       "        &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n",
       "            \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n",
       "        &\\hspace{13mm}      \\lambda \\text{ (weight decay)},  \\: \\textit{amsgrad},\n",
       "            \\:\\textit{maximize},  \\: \\epsilon \\text{ (epsilon)}                              \\\\\n",
       "        &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n",
       "            v_0\\leftarrow 0 \\text{ (second moment)},\\: v_0^{max}\\leftarrow 0          \\\\[-1.ex]\n",
       "        &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
       "        &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
       "\n",
       "        &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n",
       "        &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n",
       "        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
       "        &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n",
       "        &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
       "        &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
       "        &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n",
       "        &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n",
       "        &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n",
       "        &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n",
       "        &\\hspace{10mm} v_t^{max} \\leftarrow \\mathrm{max}(v_{t-1}^{max},v_t)                  \\\\\n",
       "        &\\hspace{10mm}\\widehat{v_t} \\leftarrow v_t^{max}/\\big(1-\\beta_2^t \\big)              \\\\\n",
       "        &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
       "        &\\hspace{10mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                  \\\\\n",
       "        &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n",
       "            \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n",
       "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
       "        &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
       "        &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
       "   \\end{aligned}\n",
       "\n",
       "For further details regarding the algorithm we refer to `Adam: A Method for Stochastic Optimization`_.\n",
       "\n",
       "Args:\n",
       "    params (iterable): iterable of parameters or named_parameters to optimize\n",
       "        or iterable of dicts defining parameter groups. When using named_parameters,\n",
       "        all parameters in all groups should be named\n",
       "    lr (float, Tensor, optional): learning rate (default: 1e-3). A tensor LR\n",
       "        is not yet supported for all our implementations. Please use a float\n",
       "        LR if you are not also specifying fused=True or capturable=True.\n",
       "    betas (Tuple[float, float], optional): coefficients used for computing\n",
       "        running averages of gradient and its square (default: (0.9, 0.999))\n",
       "    eps (float, optional): term added to the denominator to improve\n",
       "        numerical stability (default: 1e-8)\n",
       "    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
       "    decoupled_weight_decay (bool, optional): if True, this optimizer is\n",
       "        equivalent to AdamW and the algorithm will not accumulate weight\n",
       "        decay in the momentum nor variance. (default: False)\n",
       "    amsgrad (bool, optional): whether to use the AMSGrad variant of this\n",
       "        algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
       "        (default: False)\n",
       "    foreach (bool, optional): whether foreach implementation of optimizer\n",
       "        is used. If unspecified by the user (so foreach is None), we will try to use\n",
       "        foreach over the for-loop implementation on CUDA, since it is usually\n",
       "        significantly more performant. Note that the foreach implementation uses\n",
       "        ~ sizeof(params) more peak memory than the for-loop version due to the intermediates\n",
       "        being a tensorlist vs just one tensor. If memory is prohibitive, batch fewer\n",
       "        parameters through the optimizer at a time or switch this flag to False (default: None)\n",
       "    maximize (bool, optional): maximize the objective with respect to the\n",
       "        params, instead of minimizing (default: False)\n",
       "    capturable (bool, optional): whether this instance is safe to\n",
       "        capture in a graph, whether for CUDA graphs or for torch.compile support.\n",
       "        Tensors are only capturable when on supported :ref:`accelerators<accelerators>`.\n",
       "        Passing True can impair ungraphed performance, so if you don't intend to graph\n",
       "        capture this instance, leave it False (default: False)\n",
       "    differentiable (bool, optional): whether autograd should\n",
       "        occur through the optimizer step in training. Otherwise, the step()\n",
       "        function runs in a torch.no_grad() context. Setting to True can impair\n",
       "        performance, so leave it False if you don't intend to run autograd\n",
       "        through this instance (default: False)\n",
       "    fused (bool, optional): whether the fused implementation is used.\n",
       "        Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16`\n",
       "        are supported. (default: None)\n",
       "\n",
       ".. note:: The foreach and fused implementations are typically faster than the for-loop,\n",
       "          single-tensor implementation, with fused being theoretically fastest with both\n",
       "          vertical and horizontal fusion. As such, if the user has not specified either\n",
       "          flag (i.e., when foreach = fused = None), we will attempt defaulting to the foreach\n",
       "          implementation when the tensors are all on CUDA. Why not fused? Since the fused\n",
       "          implementation is relatively new, we want to give it sufficient bake-in time.\n",
       "          To specify fused, pass True for fused. To force running the for-loop\n",
       "          implementation, pass False for either foreach or fused. \n",
       ".. Note::\n",
       "    A prototype implementation of Adam and AdamW for MPS supports `torch.float32` and `torch.float16`.\n",
       ".. _Adam\\: A Method for Stochastic Optimization:\n",
       "    https://arxiv.org/abs/1412.6980\n",
       ".. _On the Convergence of Adam and Beyond:\n",
       "    https://openreview.net/forum?id=ryQu7f-RZ\n",
       "\u001b[31mFile:\u001b[39m           /opt/anaconda3/envs/cs224n/lib/python3.12/site-packages/torch/optim/adam.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     AdamW"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae7ec989-c466-4e28-99b1-edcbebd271cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38cf39f7-5539-4c15-b73c-6283b17897a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21],\n",
       "        [49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49,\n",
       "         49, 49],\n",
       "        [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2],\n",
       "        [36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
       "         36, 36],\n",
       "        [91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91,\n",
       "         91, 91],\n",
       "        [77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n",
       "         77, 77],\n",
       "        [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12],\n",
       "        [86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
       "         86, 86],\n",
       "        [28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "         28, 28],\n",
       "        [87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87,\n",
       "         87, 87],\n",
       "        [79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79,\n",
       "         79, 79],\n",
       "        [60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
       "         60, 60],\n",
       "        [86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
       "         86, 86],\n",
       "        [22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22],\n",
       "        [44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44,\n",
       "         44, 44],\n",
       "        [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2],\n",
       "        [17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "         17, 17],\n",
       "        [53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
       "         53, 53],\n",
       "        [22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22],\n",
       "        [32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
       "         32, 32],\n",
       "        [79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79,\n",
       "         79, 79],\n",
       "        [94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94,\n",
       "         94, 94],\n",
       "        [57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57,\n",
       "         57, 57],\n",
       "        [24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
       "         24, 24],\n",
       "        [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
       "         19, 19],\n",
       "        [90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90,\n",
       "         90, 90],\n",
       "        [99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
       "         99, 99],\n",
       "        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "         13, 13],\n",
       "        [28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "         28, 28],\n",
       "        [28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "         28, 28]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=idx.repeat(20,1).T\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aa8b5ac3-1a7b-4c68-945b-18b1b6d779ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors=torch.gather(embeddings,0,idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de165701-4a0a-4cc7-a4d3-7d2f1719e96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 420,  421,  422,  423,  424,  425,  426,  427,  428,  429,  430,  431,\n",
       "          432,  433,  434,  435,  436,  437,  438,  439,  980,  981,  982,  983,\n",
       "          984,  985,  986,  987,  988,  989,  990,  991,  992,  993,  994,  995,\n",
       "          996,  997,  998,  999,   40,   41,   42,   43,   44,   45,   46,   47,\n",
       "           48,   49,   50,   51,   52,   53,   54,   55,   56,   57,   58,   59],\n",
       "        [ 720,  721,  722,  723,  724,  725,  726,  727,  728,  729,  730,  731,\n",
       "          732,  733,  734,  735,  736,  737,  738,  739, 1820, 1821, 1822, 1823,\n",
       "         1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835,\n",
       "         1836, 1837, 1838, 1839, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547,\n",
       "         1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559],\n",
       "        [ 240,  241,  242,  243,  244,  245,  246,  247,  248,  249,  250,  251,\n",
       "          252,  253,  254,  255,  256,  257,  258,  259, 1720, 1721, 1722, 1723,\n",
       "         1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735,\n",
       "         1736, 1737, 1738, 1739,  560,  561,  562,  563,  564,  565,  566,  567,\n",
       "          568,  569,  570,  571,  572,  573,  574,  575,  576,  577,  578,  579],\n",
       "        [1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751,\n",
       "         1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1580, 1581, 1582, 1583,\n",
       "         1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595,\n",
       "         1596, 1597, 1598, 1599, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207,\n",
       "         1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219],\n",
       "        [1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731,\n",
       "         1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739,  440,  441,  442,  443,\n",
       "          444,  445,  446,  447,  448,  449,  450,  451,  452,  453,  454,  455,\n",
       "          456,  457,  458,  459,  880,  881,  882,  883,  884,  885,  886,  887,\n",
       "          888,  889,  890,  891,  892,  893,  894,  895,  896,  897,  898,  899],\n",
       "        [  40,   41,   42,   43,   44,   45,   46,   47,   48,   49,   50,   51,\n",
       "           52,   53,   54,   55,   56,   57,   58,   59,  340,  341,  342,  343,\n",
       "          344,  345,  346,  347,  348,  349,  350,  351,  352,  353,  354,  355,\n",
       "          356,  357,  358,  359, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067,\n",
       "         1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079],\n",
       "        [ 440,  441,  442,  443,  444,  445,  446,  447,  448,  449,  450,  451,\n",
       "          452,  453,  454,  455,  456,  457,  458,  459,  640,  641,  642,  643,\n",
       "          644,  645,  646,  647,  648,  649,  650,  651,  652,  653,  654,  655,\n",
       "          656,  657,  658,  659, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587,\n",
       "         1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599],\n",
       "        [1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891,\n",
       "         1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1140, 1141, 1142, 1143,\n",
       "         1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155,\n",
       "         1156, 1157, 1158, 1159,  480,  481,  482,  483,  484,  485,  486,  487,\n",
       "          488,  489,  490,  491,  492,  493,  494,  495,  496,  497,  498,  499],\n",
       "        [ 380,  381,  382,  383,  384,  385,  386,  387,  388,  389,  390,  391,\n",
       "          392,  393,  394,  395,  396,  397,  398,  399, 1800, 1801, 1802, 1803,\n",
       "         1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815,\n",
       "         1816, 1817, 1818, 1819, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987,\n",
       "         1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999],\n",
       "        [ 260,  261,  262,  263,  264,  265,  266,  267,  268,  269,  270,  271,\n",
       "          272,  273,  274,  275,  276,  277,  278,  279,  560,  561,  562,  563,\n",
       "          564,  565,  566,  567,  568,  569,  570,  571,  572,  573,  574,  575,\n",
       "          576,  577,  578,  579,  560,  561,  562,  563,  564,  565,  566,  567,\n",
       "          568,  569,  570,  571,  572,  573,  574,  575,  576,  577,  578,  579]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.reshape(10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e780ba41-2214-452f-b5ae-679e3583abcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993,\n",
       "        994, 995, 996, 997, 998, 999])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f70e5-a2c1-4bfa-94f4-727bba486e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
