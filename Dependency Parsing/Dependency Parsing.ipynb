{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8847335-400a-4df8-a5c9-90781abab453",
   "metadata": {},
   "source": [
    "## How to Describe the Linguistic Structure of Sentences?\n",
    "To describe the structure of sentences, people proposed two views of the linguistic structure of sentences. They are:\n",
    "\n",
    "1. Phrase Structure(Constituency) - context-free grammars\n",
    "\n",
    "2. Dependency Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f7cd7-f136-4dc0-a98e-dd776ab62427",
   "metadata": {},
   "source": [
    "### Phrase Structure\n",
    "Phrase Structure organises words into nested constituents. Words constitute phrases of different part of speech. And Phrases constitute larger phrases. For example, we have a lexicon:dog, cat, the, a, sit, on, table, cuddly, by, door... Then we can generate the following grammars:\n",
    "\n",
    "1. NP → Det (AdjP*) N (PP*)\n",
    "2. PP → P NP\n",
    "3. VP → V (NP) (PP*)\n",
    "4. S  → NP VP\n",
    "5. AdjP → Adj\n",
    "6. etc.\n",
    "\n",
    "With these phrase structure grammars, we can parse the linguistic structure of sentences. Zum Beispiel, \"A cuddly cat sits on the table by the door\" :\n",
    "1. NP1 → \"the door\" → Det + N\n",
    "2. PP1 → \"by\" + NP1\n",
    "3. NP2 → \"the table\"\n",
    "4. NP3 → NP2 + PP1\n",
    "5. PP2 → \"on\" + NP3\n",
    "6. VP1 → \"sits\" + PP2\n",
    "7. NP4 → \"a cuddly cat\" → Det + Adj + N\n",
    "\n",
    "Finally, NP4 and VP1 forms the S(entence).\n",
    "<img src=\"./phrase_structure_eg.jpg\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e2a77-5a54-4857-90a9-fc5b5198a9dc",
   "metadata": {},
   "source": [
    "### Dependency Structure\n",
    "Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words. Every word in a sentence(but one exception) depend on another word, i.e. the **\"head\"**. And the word that depends is the **\"dependent\"**. We can use directed arrows to represent this relation. Yet, there are two agreements on the direction of the arrows: **Prague direction**/**UD(Universal Dependencies)-style**(dependent→head) and **Tesnière direction**(head→dependent). In cs224n, the latter is adopted but I prefer the former, which I am gonna adopt.\n",
    "\n",
    "To parse the dependency structure of a sentence, we don't have to find all **\"heads\"** and **\"dependents\"**. Instead, we find all dependency relations. Common dependency relations are listed as follows:\n",
    "\n",
    "1. nsubj: 是head的名词性主语 (is the nominal subject of head)\n",
    "2. obj: 是head的直接宾语 (is the direct object of head)\n",
    "3. iobj: 是head的间接宾语 (is the indirect object of head)\n",
    "4. det: 是head的限定词 (is the determiner of head)\n",
    "5. amod: 是head的形容词性修饰语 (is the adjectival modifier of head)\n",
    "6. advmod: 是head的副词性修饰语 (is the adverbial modifier of head)\n",
    "7. mark: 是head的从句标记，包括引导词以及不定式、for等 (is the clause marker of head)\n",
    "8. case: 是head的格标记，例如mit是Dativ的case，für是Akusativ的case (is the case marker of head)\n",
    "9. ROOT: 依赖ROOT (dependend on the root)\n",
    "\n",
    "    ......\n",
    "\n",
    "For example, \"Look in the crate in the kitchen by the door\", we can do the following analysis:\n",
    "\n",
    "1. the $\\overset{\\text{det}}{\\longrightarrow}$ crate\n",
    "2. in $\\overset{\\text{case}}{\\longrightarrow}$ crate\n",
    "3. the $\\overset{\\text{det}}{\\longrightarrow}$ kitchen\n",
    "4. in $\\overset{\\text{case}}{\\longrightarrow}$ kitchen\n",
    "5. the $\\overset{\\text{det}}{\\longrightarrow}$ door\n",
    "6. by $\\overset{\\text{case}}{\\longrightarrow}$ door\n",
    "7. door $\\overset{\\text{nmod}}{\\longrightarrow}$ kitchen\n",
    "8. kitchen $\\overset{\\text{obl}}{\\longrightarrow}$ crate\n",
    "9. crate $\\overset{\\text{obj}}{\\longrightarrow}$ look\n",
    "10. look $\\overset{\\text{ROOT}}{\\longrightarrow}$ ROOT\n",
    "\n",
    "In the UD-style agreement that I adopt here, the words with no outward arrows, namely, the words that originally depend on no other words,  are the words to point to a fake ROOT. The word that should point to a ROOT here is \"look\". Why we need a fake root? Because a sentence may have multiple core words, to draw a strictly defined tree rather than a forest, a ROOT is need to connect different core words.\n",
    "<center>\n",
    "<img src=\"./dependency_structure_eg.jpeg\" width=\"400\" height=\"300\">\n",
    "</center>\n",
    "And then we can draw a tree graph which is connected, acyclic and has a single root. This gives us the dependency tree analysis.(I missed a ROOT here which should be the destination of \"look\")\n",
    "<center>\n",
    "<img src=\"./dependency_tree_graph.jpg\" width=\"200\" height=\"100\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc3be9-3a7e-4b6f-a03f-b74c4f4ee20a",
   "metadata": {},
   "source": [
    "### From Grammar Rules to Annotated Data\n",
    "Dependency Structure proved to be more suitable for data-driven NLP. Here's a teeny introduction of the history:\n",
    "\n",
    "In early times, linguists would write specific dependency grammars just to build one particular parser. And when the parser is finished, people evaluate it very subjectively. For example, you type in a sentence and see what the parser outputs. And then you stare at it and contemplate \"Umm it looks fairly good\" or \"Well it's a piece of sh*t\".\n",
    "\n",
    "The advent of treebanks fundamentally changed this process.\". If we build a treebank with annotated dependency parsing data, we can reuse the treebank to build parsers, part-of-speech tagger, etc. In addition, a treebank can also function as an evalution, prodiving a more quantitative perspective. It is sort of like the concept of \"test set\" in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73234022-a675-44c6-bc63-057cb86e6644",
   "metadata": {},
   "source": [
    "### Learning to Parse with Treebank\n",
    "For now, we have a powerful treebank [UD](https://universaldependencies.org). Yet, directly using the original features provided by the treebank doesn't give us satisfying performance. Naturally, we want to utilise linguistic prior knowledge, i.e., sources of information:\n",
    "\n",
    "1. Bilexical affinities:\n",
    "2. Dependency distance:\n",
    "3. Intervening material:\n",
    "4. Valency of heads: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b232fb-5891-4bff-9f80-e1d1dcb30b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
