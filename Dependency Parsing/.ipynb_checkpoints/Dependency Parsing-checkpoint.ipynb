{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8847335-400a-4df8-a5c9-90781abab453",
   "metadata": {},
   "source": [
    "## How to Describe the Linguistic Structure of Sentences?\n",
    "To describe the structure of sentences, people proposed two views of the linguistic structure of sentences. They are:\n",
    "\n",
    "1. Phrase Structure(Constituency) - context-free grammars\n",
    "\n",
    "2. Dependency Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f7cd7-f136-4dc0-a98e-dd776ab62427",
   "metadata": {},
   "source": [
    "### Phrase Structure\n",
    "Phrase Structure organises words into nested constituents. Words constitute phrases of different part of speech. And Phrases constitute larger phrases. For example, we have a lexicon:dog, cat, the, a, sit, on, table, cuddly, by, door... Then we can generate the following grammars:\n",
    "\n",
    "1. NP → Det (AdjP*) N (PP*)\n",
    "2. PP → P NP\n",
    "3. VP → V (NP) (PP*)\n",
    "4. S  → NP VP\n",
    "5. AdjP → Adj\n",
    "6. etc.\n",
    "\n",
    "With these phrase structure grammars, we can parse the linguistic structure of sentences. Zum Beispiel, \"A cuddly cat sits on the table by the door\" :\n",
    "1. NP1 → \"the door\" → Det + N\n",
    "2. PP1 → \"by\" + NP1\n",
    "3. NP2 → \"the table\"\n",
    "4. NP3 → NP2 + PP1\n",
    "5. PP2 → \"on\" + NP3\n",
    "6. VP1 → \"sits\" + PP2\n",
    "7. NP4 → \"a cuddly cat\" → Det + Adj + N\n",
    "\n",
    "Finally, NP4 and VP1 forms the S(entence).\n",
    "<img src=\"./phrase_structure_eg.jpg\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e2a77-5a54-4857-90a9-fc5b5198a9dc",
   "metadata": {},
   "source": [
    "### Dependency Structure\n",
    "Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words. Every word in a sentence has at most one head, except for the root word, which has no head. And the word that depends is the **\"dependent\"**. We can use directed arrows to represent this relation. Yet, there are two conventions on the direction of the arrows: **UD(Universal Dependencies)-style**(head→dependent) and **Tesnière direction**(dependent→head). In cs224n, the former is adopted, which I am gonna adopt.\n",
    "\n",
    "To parse the dependency structure of a sentence, we find all dependency relations. Common dependency relations are listed as follows:\n",
    "\n",
    "1. nsubj: 是head的名词性主语 (is the nominal subject of head)\n",
    "2. obj: 是head的直接宾语 (is the direct object of head)\n",
    "3. iobj: 是head的间接宾语 (is the indirect object of head)\n",
    "4. det: 是head的限定词 (is the determiner of head)\n",
    "5. amod: 是head的形容词性修饰语 (is the adjectival modifier of head)\n",
    "6. advmod: 是head的副词性修饰语 (is the adverbial modifier of head)\n",
    "7. mark: 是head的从句标记，包括引导词以及不定式、for等 (is the clause marker of head)\n",
    "8. case: 是head的格标记，例如mit是Dativ的case，für是Akusativ的case (is the case marker of head)\n",
    "9. root: 依赖ROOT (the dependency relation between the root word and the artificial ROOT node)\n",
    "\n",
    "    ......\n",
    "\n",
    "For example, \"Look in the crate in the kitchen by the door\", we can do the following analysis:\n",
    "\n",
    "1. crate  $\\overset{\\text{det}}{\\longrightarrow}$ the\n",
    "2. crate  $\\overset{\\text{case}}{\\longrightarrow}$ in\n",
    "3. kitchen $\\overset{\\text{det}}{\\longrightarrow}$ the\n",
    "4. kitchen $\\overset{\\text{case}}{\\longrightarrow}$ in\n",
    "5. door   $\\overset{\\text{det}}{\\longrightarrow}$ the\n",
    "6. door   $\\overset{\\text{case}}{\\longrightarrow}$ by\n",
    "7. kitchen $\\overset{\\text{nmod}}{\\longrightarrow}$ door\n",
    "8. crate  $\\overset{\\text{obl}}{\\longrightarrow}$ kitchen\n",
    "9. look   $\\overset{\\text{obj}}{\\longrightarrow}$ crate\n",
    "10. ROOT  $\\overset{\\text{root}}{\\longrightarrow}$ look\n",
    "\n",
    "\n",
    "The words with no outward arrows, namely, the words that originally depend on no other words,  are the words to point to a fake ROOT. The word that should point to a ROOT here is \"look\". Why we need a fake root? Because a sentence may have multiple core words, which causes multiple trees. To draw a strictly defined tree rather than a forest, a ROOT is needed to connect different core words.\n",
    "<center>\n",
    "<img src=\"./dependency_structure_eg.jpeg\" width=\"400\" height=\"300\">\n",
    "<br>\n",
    "<em>Dependencies of the Example Sentence. (Note: the arrow directions in this figure are reversed by mistake.)</em>\n",
    "</center>\n",
    "And then we can draw a tree graph which is connected, acyclic and has a single root. This gives us the dependency tree analysis.(I missed a ROOT here which should be the destination of \"look\")\n",
    "<center>\n",
    "<img src=\"./dependency_tree_graph.jpg\" width=\"200\" height=\"100\">\n",
    "<br>\n",
    "<em>Dependency Tree of the Example Sentence. (Note: the arrow directions in this figure are reversed by mistake.)</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc3be9-3a7e-4b6f-a03f-b74c4f4ee20a",
   "metadata": {},
   "source": [
    "### From Grammar Rules to Annotated Data\n",
    "Dependency Structure proved to be more suitable for data-driven NLP. Here's a teeny introduction to the history:\n",
    "\n",
    "In early times, linguists would write specific dependency grammars just to build one particular parser. And when the parser is finished, people evaluate it very subjectively. For example, you type in a sentence and see what the parser outputs. And then you stare at it and contemplate \"Umm it looks fairly good\" or \"Well it's a piece of sh*t\".\n",
    "\n",
    "The advent of treebanks fundamentally changed this process. If we build a treebank with annotated dependency parsing data, we can reuse the treebank to build parsers, part-of-speech taggers, etc. In addition, a treebank can also function as an evaluation, providing a more quantitative perspective. It is sort of like the concept of \"test set\" in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73234022-a675-44c6-bc63-057cb86e6644",
   "metadata": {},
   "source": [
    "### Learning to Parse with Treebank\n",
    "For now, we have a powerful treebank [UD](https://universaldependencies.org). Yet, directly using the original features provided by the treebank doesn't give us satisfying performance. Naturally, we want to utilise linguistic prior knowledge, i.e., sources of information:\n",
    "\n",
    "1. Bilexical affinities: how plausible a dependency is, e.g. \"apple → read\" is not plausible while \"apple → eat\" is.\n",
    "2. Dependency distance: most dependencies are between nearby words.\n",
    "3. Intervening material: dependencies rarely span across verbs or punctuation. That is to say the words that a dependency arrow spans across are usually not verbs or punctuation.\n",
    "4. Valency of heads: how many dependent words on which side are usually for a head. For example, for the head \"eat\", its left valency is usually 1, which is the subject of \"eat\", and its right valency is also usually 1, which is the object of \"eat\". So the total valency of \"eat\" is usually 2.\n",
    "\n",
    "Then how do we use the information to build a parser? What we are gonna do is for each word choose what other word (including ROOT) it is dependent of. And there are usually some constraints:\n",
    "\n",
    "1. Only one word is a dependent of ROOT.\n",
    "2. No cycles, e.g., A→B, B→A.\n",
    "3. Most of the time, dependencies are projective, i.e., dependency arcs don't cross.\n",
    "\n",
    "#### Projectivity\n",
    "Definition: There are no crossing dependency arcs when the words are laid out in their linear order, with all arcs above the words.\n",
    "\n",
    "Law: Dependencies derived from a projective CFG parse tree must be projective.\n",
    "\n",
    "### Methods of Parsing\n",
    "\n",
    "1. Dynamic Programming: Complexity $O(n^3)$.\n",
    "2. Graph algorithms: Minimum Spanning Tree; Neural graph-based parser.\n",
    "3. Constraint satisfaction parser.\n",
    "4. Transition-based parsing or deterministic dependency parsing.\n",
    "\n",
    "### Greedy Transition Based Parser\n",
    "a basic transition-based parser has 4 components:\n",
    "\n",
    "1. a stack $\\sigma$ = [ROOT]\n",
    "2. a buffer $\\beta$ = the token sequence of the sentence\n",
    "3. a set of dependency arcs $A$ = {}\n",
    "4. a set of actions: Shift, Left-Arc, Right-Arc\n",
    "\n",
    "N.B: Left-Arc and Right-Arc are only allowed when the dependent does not already have a head, and ROOT cannot be a dependent.\n",
    "\n",
    "#### Algorithm Description(arc-standard):\n",
    "\n",
    "First, we initialize the stack $\\sigma$ as [ROOT], and put the sentence’s tokens into the buffer $\\beta$ in their original word order. Then we initialize an empty set of dependency arcs $A$.\n",
    "\n",
    "Next, a classifier chooses among three possible actions:\n",
    "\n",
    "1. Shift: Move the leftmost word in the buffer $\\beta$ to the right end of the stack $\\sigma$.\n",
    "\n",
    "2. Left-Arc: Create a dependency arc from the top of the stack (the rightmost word in the stack) to the second-top word (the second rightmost word). In this arc, the left word is the dependent and the right word is the head. Remove the dependent from the stack and add the newly created arc to $A$.\n",
    "\n",
    "3. Right-Arc: Create a dependency arc from the second-top word in the stack to the top word. In this arc, the left word is the head and the right word is the dependent. Remove the dependent from the stack and add the newly created arc to $A$.\n",
    "\n",
    "This process continues until the buffer $\\beta$ is empty and only ROOT remains in the stack $\\sigma$.\n",
    "\n",
    "首先，我们初始化$\\sigma$为[ROOT]，并将句子的token序列按句子排列顺序放入buffer $\\beta$，然后初始化一个空的依存弧$A$。\n",
    "\n",
    "接着，由分类器决定进行三种动作:\n",
    "\n",
    "1. Shift：将buffer $\\beta$中的最左边的词推入stack $\\sigma$的最右边。\n",
    "2. Left-Arc：建立由栈顶词（栈最右边的词）指向次栈顶词（栈次最右边的词）的依存弧，弧的左词为dependent，右词为head；并将被指向的词移出栈，然后将建立的依存弧添加到$A$。\n",
    "3. Right-Arc：建立由次顶词（栈次最右边的词）指向栈顶词（栈最右边的词）的依存弧，弧的左词为head，右词为dependent。并将被指向的词移出栈，然后将建立的依存弧添加到$A$。\n",
    "\n",
    "直到：buffer $\\beta$为空且stack $\\sigma$内只剩ROOT。\n",
    "\n",
    "**Example: Analysis of \"I ate fish\"**\n",
    "1. $\\sigma$=[ROOT], $\\beta$=[I, ate, fish], $A$={}\n",
    "2. Since there is only ROOT in $\\sigma$, we shift one word \"I\": $\\sigma$=[ROOT, I], $\\beta$=[ate, fish], $A$={}.\n",
    "3. Since ROOT cannot be a dependent, action Left-Arc is not allowed. Besides, \"I\" is obviously not the core of the sentence so we don't do Right-Arc, we shift another word: $\\sigma$=[ROOT, I, ate], $\\beta$=[fish], $A$={}.\n",
    "4. \"I\" is the dependent of \"ate\", we do a Left-Arc action: $\\sigma$=[ROOT, ate], $\\beta$=[fish], $A$={ate→I}.\n",
    "5. We could do a Right-Arc action again but that would be the wrong thing to do. So we shift: $\\sigma$=[ROOT, ate, fish], $\\beta$=[], $A$={ate→I}.\n",
    "6. Obviously, we do Right-Arc: $\\sigma$=[ROOT, ate], $\\beta$=[], $A$={ate→I, ate→fish}.\n",
    "7. Finally, Right-Arc again: $\\sigma$=[ROOT], $\\beta$=[], $A$={ate→I, ate→fish, ROOT→ate}.\n",
    "\n",
    "#### Feature Representation\n",
    "\n",
    "#### Evaluation of Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b232fb-5891-4bff-9f80-e1d1dcb30b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
