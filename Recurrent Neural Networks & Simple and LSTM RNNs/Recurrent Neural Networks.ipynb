{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380d250f-20f9-4adc-a773-5bf479a27e5d",
   "metadata": {},
   "source": [
    "## 1. Language Modelling\n",
    "Language Modelling is modelling the probability of a sequence: \n",
    "$$\n",
    "P(w_1, w_2, ..., w_n)=P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2)...P(w_n|w_1,w_2,...,w_{n-1})=\\prod_{t=1}^{T}P(w_t|w_1,w_2,...,w_{t-1})\n",
    "$$\n",
    "An early statistical language model is the **n-gram model**. It assumes that the nth word is dependent only on the preceding n-1 words to make probability computation feasible. For example, a bi-gram model use one preceding word to predict the next word, a tri-gram model uses two preceding words to predict the next word...\n",
    "\n",
    "But with n-gram model, we generate grammartical but incoherent texts that don't make sense. So let's see neural language models.\n",
    "\n",
    "## 2. How to Build Neural Language Models(NLM)\n",
    "### 2.1 A fixed-window NLM\n",
    "It follows the convention of n-gram---predict the nth word given preceding n-1 words. Word vectors are concatenated into a window vector. And it receives the window vector, processes it with an MLP and outputs softmax probabilities over the entire vocabulary. However, one big problem of this model is that it cannot handle variable sequence length dependency. So we'll see RNN.\n",
    "\n",
    "### 2.2 RNN\n",
    "#### 2.2.1 Model Structure\n",
    "RNN is like a reusable MLP. An RNN receives two inputs every step. We input a vector $x_1$ (it can be the first word vector of a sentence), and initialise a hidden state $h_0$ (usually a zero vector), take their respective affine transformations $W_{xh}x_1$ and $W_{hh}h_0$, and then plus a bias $b_h$, sum them up, we get $W_{xh}x_1+W_{hh}h_0+b_h$. And then we apply an activation function $\\Phi$, we get $\\Phi(W_{xh}x_1+W_{hh}h_0+b_h)$. This is what we use to compute the current hidden state $h_1$. To get an output of the current time step, just do an affine transformation with $h_1$ and add a bias $b_o$, apply an activation function. We can do this calculation recurrently: input $x_2$ (the second word vector of a sentence), load $h_1$...... This is how an RNN works. The main maths are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_t=\\Phi(W_{xh}x_t+W_{hh}h_{t-1}+b_h) \\\\\n",
    "o_t=\\phi(W_{ho}h_t+b_o)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "I also have a diagram:\n",
    "<center>\n",
    "<img src=\"./RNN.png\" width=500 height=500>\n",
    "</center>\n",
    "\n",
    "#### 2.2.2 Training an RNN\n",
    "\n",
    "Let's say we are doing a Next-Word-Prediction task and we have a huge corpus starting with \"the students opened their exams\". When we input \"the\", we get the prob of the next word. However, we don't choose the most probable word as the next input. Instead, we input the ground truth \"students\". This is called \"teacher forcing\", aiming to avoid error accumulation, et cetera. And then input \"opened\"... Every prediction, we compute a loss. In practice, we don't just run through the entire corpus and compute all losses. It's too computationally expensive. Rather, we split the corpus into sentences( or documents), put them into batches. In a batch of sentences, losses of words of different sentences are computed in parallel. To do backprob, we compute the average losses of every sentence i.e., $\\frac{loss_{w1}+loss_{w2}+...}{num\\_words}$, then average the sentence-level losses, i,e., $\\frac{loss_{s1}+loss_{s2}+...}{batch\\_size}$, finally do mini batch gradient descent. Well, this is equivalent to averaging the losses of all tokens in a batch, which is what we in practice do.<a id=\"one\"></a>\n",
    "\n",
    "##### 2.2.2.1 Gradient Calculation:\n",
    "The gradient of loss at time step $t$ w.r.t. recurrent weights is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J^{(t)}}{\\partial W_{hh}}\n",
    "&= \\sum_{i=1}^{t}{\\frac{\\partial J^{(t)}}{\\partial W_{hh}}} \\Big|_{(i)} \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{hh}} \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot (\\frac{\\partial h_t}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial W_{hh}}) \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot (\\frac{\\partial h_t}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot (\\frac{\\partial h_{t-1}}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdot \\frac{\\partial h_{t-2}}{\\partial W_{hh}})) \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot (\\frac{\\partial h_t}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot (\\frac{\\partial h_{t-1}}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdot (\\frac{\\partial h_{t-2}}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial h_{t-2}}{\\partial h_{t-3}} \\cdot \\frac{\\partial h_{t-3}}{\\partial W_{hh}}))) \\\\\n",
    "&= \\text{...} \\\\\n",
    "&= \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_{hh}} \\Big|_{dir} + \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial W_{hh}} \\Big|_{dir} + ... + \\frac{\\partial J^{(t)}}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdot ... \\cdot \\frac{\\partial h_{2}}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial W_{hh}} \\Big|_{dir}\n",
    "\\tag{1}\n",
    "\\end{align}\n",
    "$$\n",
    "**N.B: The sign \"$\\Big|_{dir}$\" denotes the direct partial derivative. When calculating direct partial derivatives, treat intermediate variables as constants.**\n",
    "\n",
    "To get the gradient of the loss over the entire sequence, sum the gradients contributed by all time steps:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W_{hh}}=\\sum^{T}_{t=1}{\\frac{\\partial J^{(t)}}{\\partial W_{hh}}}\n",
    "$$\n",
    "\n",
    "As for the gradient of loss at time step $t$ w.r.t. **input weights**, the form is the same as above, just replace $W_{hh}$ with $W_{xh}$. And the gradient of loss w.r.t. **output weights**, they don't depend on time steps thus easy to calculate.\n",
    "\n",
    "##### 2.2.2.2 Gradient Clipping\n",
    "As you can see from the formula (1), the length of matrix mulplication chain is O(t), which is prone to causing gradient instability problems. For gradient vanishing, there's no general solution. But for gradient exploding, we have **Gradient Clipping**. The main idea is: Put all gradients into a long one dimensional vector $g$, calculate its L2 norm, which reflects the magnitude of the overall gradient. If it's too large, clip it.\n",
    "\n",
    "More specifically, we have gradient matrices: $\\partial W_{xh}$, $\\partial W_{hh}$ and $\\partial W_{ho}$ (biases not considered for simplification). We take all gradient values to form a vector $g$ in the shape of (x*h+h*h+h*o), calculate its L2 norm $||g||$. Next we compare this value with a threshold $\\theta$. $||g||$ being larger than $\\theta$ means that gradients need to be clipped(scaled). We scale $g$ by $\\frac{\\theta}{||g||}$. If not, we do nothing or just consider $\\theta$ to be 1.\n",
    "\n",
    "$$\n",
    "g←min(1, \\frac{\\theta}{||g||}) \\cdot g\n",
    "$$\n",
    "\n",
    "Doing so, we scale down exploding gradients to relatively moderate values. So $g$ are the gradients with which we do weight updates.\n",
    "\n",
    "**My Comtemplation**:Can batch normalization be applied between the time steps of an RNN? My idea is based on the fact that gradients closer to the loss function are less affected by gradient vanishing and exploding. In MLPs, this issue can be alleviated by adding batch normalization layers between the fully connected layers. So, in an RNN, can batch normalization be added between time steps to alleviate this problem?\n",
    "\n",
    "The answer is: not batch norm, but layer norm. And Layer Norm can alleviate both gradient vanishing（mainly） and gradient exploding(slightly).\n",
    "\n",
    "But overall, the most effective way is to abandon RNN and turn to LSTM.\n",
    "\n",
    "#### 2.2.3 Applications\n",
    "##### Generating Texts\n",
    "Input the first word, get the probabilities of the next word, sample one word as the input for the next step... Recurrently.\n",
    "\n",
    "##### Sequence Tagging\n",
    "This includes part of speech tagging, Named Entity Recognition, etc. For example, input \"The startled cat knocked over the vase\", output their part of speech (DT, JJ, NN, VBN, IN, DT, NN)\n",
    "\n",
    "##### Sentiment(Text) Classification\n",
    "Normally, we can connect RNN hidden states of all time steps to an nn.Linear producing an output for each time step. But if we only want one output such as the probability vector, we connect the last hidden state of the last time step to an nn.Linear. Yet, there is a usually better way, which takes advantage of hidden states of all time steps. We take the hidden state vectors, and apply a function such as mean or max to pool the them so that we get one vector, and then put it into an nn. Linear.\n",
    "\n",
    "##### Language Encoder\n",
    "The hidden states of RNNs are encoded features of input sequences. We can employ them to do many other tasks such as machine translation, question answering, etc.\n",
    "\n",
    "##### Signal(Feature) Decoder\n",
    "Decode features of signals. For instance, input audio signals, and use RNNs to decode them into sequences of words; decode the encoded features in tasks like machine translation, summarization, etc. This is the concept of conditional langauge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f15ae5f-7e19-4886-95dc-359197b36246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN implementation from scratch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "def get_params(num_inputs, num_hiddens, num_outputs): # the size of input layer, hidden layer and output layer.\n",
    "\n",
    "    def norm(shape):\n",
    "        return torch.randn(size=shape) * 0.01\n",
    "\n",
    "    W_xh=norm((num_inputs, num_hiddens))\n",
    "    W_hh=norm((num_hiddens, num_hiddens))\n",
    "    W_ho=norm((num_hiddens, num_outputs))\n",
    "    b_h=torch.zeros((num_hiddens))\n",
    "    b_o=torch.zeros((num_outputs))\n",
    "    params=[W_xh, W_hh, b_h, W_ho, b_o]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params\n",
    "\n",
    "def init_rnn_hidden_state(batch_size, num_hiddens):\n",
    "    return torch.zeros((batch_size, num_hiddens))\n",
    "\n",
    "def rnn(inputs, init_state, params):\n",
    "    W_xh, W_hh, b_h, W_ho, b_o=params\n",
    "    H=init_state #shape: (batch_size, num_hiddens)\n",
    "    outputs=[]\n",
    "    for X in inputs:# shape of inputs: (sequence_length, batch_size, num_inputs/input_dim)\n",
    "        H=torch.tanh(X@W_xh + H@W_hh + b_h)\n",
    "        O=H@W_ho + b_o #shape:(batch_size, num_outputs)\n",
    "        outputs.append(O) # list of (batch_size, num_outputs) with a length of \"sequence_length\"\n",
    "    return (torch.concat(outputs, dim=0), #shape: (sequence_length*batch_size, num_outputs)\n",
    "           H) #last_hidden_state\n",
    "\n",
    "class RNNFromScratch():\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, get_params, init_rnn_hidden_state, forward_fn):\n",
    "        self.num_inputs, self.num_hiddens, self.num_outputs=num_inputs, num_hiddens, num_outputs\n",
    "        self.params=get_params(self.num_inputs, self.num_hiddens, self.num_outputs)\n",
    "        self.init_rnn_hidden_state, self.forward_fn=init_rnn_hidden_state, forward_fn\n",
    "\n",
    "    def __call__(self, inputs, init_state):\n",
    "        #shape of inputs: (sequence_length, batch_size, num_inputs), so you need to do the encoding yourself.\n",
    "        return self.forward_fn(inputs, init_state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size):\n",
    "        #Here I choose to relinquish the right to decide when to init RNN state.\n",
    "        return self.init_rnn_hidden_state(batch_size, self.num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf6a940-782a-40be-9640-94f6ef58bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I'm gonna show you how the model does a forwarding.\n",
    "I'll generate a three-word(sequence_length=3) four-sentence(batch_size=4) corpus,\n",
    "where each word is embedded into ten-dimensional(vocab_size=10).\n",
    "And the model suits the data, it has an num_input of 10. And the num_hiddens of 20 is randomly set. It outputs 10-dimensional vectors.\n",
    "'''\n",
    "X=torch.randn((3,4,10))\n",
    "model=RNNFromScratch(10, 20, 10, get_params, init_rnn_hidden_state, rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ac21ef-d6f7-4956-8421-41ae656acd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state=model.begin_state(4) #4=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c102e44f-973d-45f0-926e-6443335dbaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 10]),\n",
       " torch.Size([4, 20]),\n",
       " tensor([[-1.1979e-03,  5.1540e-03,  1.1129e-03,  3.3862e-03, -1.7307e-03,\n",
       "           2.0465e-03, -5.0387e-03, -2.1164e-03,  5.9895e-03,  1.0317e-03],\n",
       "         [ 7.8390e-05, -2.8698e-03,  4.6500e-04, -1.4766e-03,  1.4919e-03,\n",
       "          -4.1913e-04,  4.8445e-04, -7.3161e-04, -1.8116e-03,  5.3989e-04],\n",
       "         [ 3.5176e-04,  6.4057e-04,  3.1082e-04,  6.3043e-04, -4.8229e-04,\n",
       "          -6.3009e-04,  2.4773e-03,  1.6382e-03, -3.4627e-04, -1.3033e-04],\n",
       "         [ 2.3999e-04,  3.2863e-03, -1.2607e-03,  6.2587e-05, -1.5069e-03,\n",
       "          -2.9013e-03,  3.6985e-03,  2.8612e-03, -2.4104e-03, -3.3594e-03],\n",
       "         [-2.9506e-04,  2.7968e-04,  1.0231e-03,  1.6583e-03,  6.9526e-04,\n",
       "          -2.2222e-04,  1.3829e-03,  1.3393e-03, -5.5961e-04, -3.0589e-05],\n",
       "         [ 2.4815e-04,  6.5018e-04, -5.8059e-04,  4.7557e-05, -1.0589e-03,\n",
       "           1.1735e-03, -1.5111e-03,  1.2587e-04,  1.1971e-03,  1.5849e-03],\n",
       "         [-8.6718e-05, -1.8525e-03,  1.0836e-03,  8.9050e-04,  2.3081e-03,\n",
       "           1.2532e-03, -1.1177e-03, -6.5754e-04, -1.7921e-03,  1.3241e-03],\n",
       "         [ 5.7539e-05,  6.1366e-04, -1.2595e-03, -7.4777e-04, -1.1950e-03,\n",
       "           5.3698e-05, -7.1741e-04,  6.6426e-04, -5.4594e-04,  4.6427e-04],\n",
       "         [ 6.8276e-05, -1.9373e-03, -6.4170e-05, -4.4100e-04,  2.2602e-03,\n",
       "          -5.8030e-04,  1.3818e-03,  1.3463e-03, -3.4301e-03, -1.3851e-03],\n",
       "         [ 7.2149e-04, -2.1391e-03, -4.9810e-04, -1.8086e-03, -7.3904e-04,\n",
       "          -2.9184e-03,  5.3392e-03,  8.1864e-04, -1.1431e-03, -1.3931e-03],\n",
       "         [-5.1441e-04, -1.0193e-03,  1.3169e-03,  5.2364e-04,  7.6803e-04,\n",
       "           7.5483e-04, -1.9486e-03, -1.7997e-03,  1.8584e-03,  1.9202e-03],\n",
       "         [ 6.2756e-04, -2.9204e-03, -4.0062e-04, -7.8698e-04,  8.2286e-04,\n",
       "          -5.3189e-04,  2.1774e-03,  3.0715e-04, -1.2777e-03, -1.6360e-04]],\n",
       "        grad_fn=<CatBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred, last_hidden_state=model(X, state)\n",
    "Y_pred.shape, last_hidden_state.shape, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4f07f4-d733-4fad-9bb2-3a04ca24ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_clipping(model, theta):\n",
    "    if isinstance(model, nn.Module):\n",
    "        params=[p for p in model.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params=[p for p in model.params]\n",
    "    norm=torch.sqrt(sum(torch.sum(p.grad**2) for p in params))# Here we don't fisrt concat then calculate. Instead, we calculate then sum.\n",
    "    if norm>theta:\n",
    "        for param in params:\n",
    "            param.grad[:]*=theta/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "698b8180-a9e1-452a-916e-926a9c2b0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "Y=torch.randint(0,10,(12,))\n",
    "optimizer=torch.optim.Adam(model.params)\n",
    "criterion=torch.nn.CrossEntropyLoss()#reduction=\"mean\" by default. So we are summing the loss of every vector from every sentence(batch)\n",
    "#and average it.(just like what I said in the 1st paragraph under \"Trainging an RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d86d24-61a3-4875-a9e9-31039f9f4a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1063e-02, -4.9451e-03, -3.3603e-03, -1.4943e-02,  1.2321e-02,\n",
       "          7.2303e-03, -4.3888e-03, -1.2437e-02,  9.4791e-03,  6.2328e-03,\n",
       "          1.0857e-02, -6.0974e-03,  8.3845e-03, -6.0673e-03, -6.0042e-03,\n",
       "         -1.2711e-02,  1.1402e-03,  5.0844e-03, -1.8377e-03,  5.2884e-03],\n",
       "        [ 4.6487e-03,  5.0974e-03, -5.4588e-04, -1.1416e-02, -1.5981e-02,\n",
       "          1.9954e-02,  1.0117e-02,  1.2759e-04, -1.1237e-02, -4.8335e-03,\n",
       "         -1.0077e-02, -4.3125e-03, -1.8279e-03, -1.6127e-02,  7.3236e-03,\n",
       "         -6.3227e-03,  3.2423e-03,  6.3073e-03, -6.1960e-03, -9.8097e-03],\n",
       "        [-6.1425e-03,  1.2879e-03,  2.8751e-03, -6.0438e-04, -5.8868e-03,\n",
       "          1.3768e-04,  7.6099e-03, -2.7376e-03,  1.1428e-02, -9.7757e-03,\n",
       "          3.7488e-03,  5.5861e-03,  1.7756e-03, -1.1226e-02,  1.2693e-02,\n",
       "          2.4834e-03,  8.7297e-04,  5.2799e-03, -1.2797e-02,  4.8067e-03],\n",
       "        [-5.1562e-03, -7.5347e-03, -1.1579e-02, -5.4252e-03, -2.0262e-02,\n",
       "          2.7238e-04,  1.3603e-02, -6.6191e-03,  5.5128e-03, -9.9544e-04,\n",
       "          6.9564e-03,  2.4534e-03, -9.9566e-03, -8.5351e-03,  4.6153e-03,\n",
       "          2.2424e-02, -1.2685e-02, -2.8738e-03,  8.4245e-04,  4.2653e-04],\n",
       "        [-1.3983e-02, -8.7411e-03, -5.1312e-03,  1.4097e-02,  4.5684e-03,\n",
       "          2.0020e-02,  1.3711e-04, -2.3786e-04,  8.1327e-03,  8.6093e-03,\n",
       "          1.2432e-03, -1.9685e-03, -3.8994e-04, -1.3636e-02, -2.9897e-03,\n",
       "         -6.7714e-03, -7.7118e-03,  1.4489e-02, -1.5002e-02, -1.1435e-02],\n",
       "        [ 4.4714e-04, -1.3676e-02,  7.8315e-03, -6.8098e-04,  6.9985e-03,\n",
       "          1.6058e-02, -1.3197e-02,  1.8310e-02, -4.3610e-03, -2.5167e-03,\n",
       "         -1.9500e-02, -8.4361e-03, -9.9016e-03,  8.1251e-03, -9.2113e-04,\n",
       "         -8.1756e-03,  9.2332e-03, -1.4460e-02,  1.3029e-02,  9.7038e-03],\n",
       "        [-7.8014e-03, -2.6963e-03, -6.8968e-03,  1.9852e-02, -1.6492e-02,\n",
       "          2.3467e-05, -1.3048e-02,  3.7002e-03, -4.4995e-03, -1.2653e-02,\n",
       "          3.8579e-04,  3.2113e-03, -3.4753e-03, -1.8871e-02,  6.7249e-03,\n",
       "         -8.5714e-03, -7.8468e-03,  2.4669e-03, -7.8314e-03,  6.9257e-04],\n",
       "        [-1.0536e-02, -4.7977e-03, -6.4991e-03,  2.5664e-02, -6.6242e-04,\n",
       "         -8.6097e-03,  1.3057e-02, -4.7972e-03,  4.4964e-03, -1.8420e-02,\n",
       "         -1.4232e-02, -6.3462e-03, -6.1032e-03, -7.3839e-03, -5.4518e-04,\n",
       "         -1.3650e-02,  1.8920e-03,  1.4553e-02, -2.6229e-03,  9.1248e-03],\n",
       "        [ 9.1817e-03,  2.6600e-03,  6.9691e-03,  1.9350e-03,  2.2773e-03,\n",
       "          2.6253e-03,  1.3383e-02, -1.5583e-02,  3.0587e-03,  1.7518e-02,\n",
       "         -2.3708e-03, -7.7781e-03,  6.6515e-03,  8.3429e-03,  6.2604e-03,\n",
       "          8.4023e-04, -2.7681e-04,  1.1901e-02, -6.6065e-03, -1.1002e-03],\n",
       "        [-8.3899e-03,  2.5022e-03,  2.3430e-03,  9.2367e-03, -1.8767e-03,\n",
       "          2.4693e-03, -1.6823e-02, -1.6234e-03, -7.0221e-03,  3.8456e-03,\n",
       "          2.8437e-03, -4.0344e-03, -1.1757e-02,  3.6355e-03,  1.3120e-02,\n",
       "         -1.4615e-02,  2.3299e-03, -1.1901e-02,  3.5061e-04, -1.8600e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred, last_hidden_state=model(X, state)\n",
    "optimizer.zero_grad()\n",
    "loss=criterion(Y_pred,Y)\n",
    "loss.backward()\n",
    "gradient_clipping(model, 1.0)\n",
    "optimizer.step()\n",
    "model.params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9729f0-66bc-4d06-b17c-33f6465f3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN implementation with torch api\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim , num_hiddens):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.rnn=nn.RNN(embedding_dim, num_hiddens, batch_first=True) #num_hidden_layer is 1 by default. \n",
    "        #And h0 is by default torch.zeros((num_hiden_layers, batch_size, num_hiddens))\n",
    "        self.fc=nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def forward(self, X): # X in the shape of (batch_size, seq_length)\n",
    "        embeddings=self.embedding(X) #shape: (batch_size, seq_length, embedding_dim)\n",
    "        hidden_states, h_n=self.rnn(embeddings)\n",
    "        logits=self.fc(hidden_states.reshape(-1, hidden_states.size(-1)))\n",
    "        logits=logits.reshape((hidden_states.size(0), hidden_states.size(1), -1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ebcf874-9e72-49a9-9b1c-e372a388d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''Some of the most important decisions of our lives occur while we are feeling stressed and anxious. From medical decisions to financial and professional ones, we are all sometimes required to weigh up information under stressful conditions. But do we become better or worse at processing and using information under such circumstances? My colleague and I, both neuroscientists, wanted to investigate how the mind operates under stress, so we visited some local fire stations. Firefighters' workdays vary quite a bit. Some are pretty relaxed; they will spend their time washing the truck, cleaning equipment, cooking meals and reading. Other days can be hectic, with numerous life threatening incidents to attend to; they will enter burning homes to rescue trapped residents, and assist with medical emergencies. These ups and downs presented the perfect setting for an experiment on how people's ability to use information changes when they feel under pressure. We found that perceived threat acted as a trigger for a stress reaction that made the task of processing information easier for the firefighters - but only as long as it conveyed bad news. This is how we arrived at these results. We asked the firefighters to estimate their likelihood of experiencing 40 different adverse events in their life, such as being involved in an accident or becoming a victim of card fraud. We then gave them either good news (that their likelihood of experiencing these events was lower than they'd thought) or bad news (that it was higher) and asked them to provide new estimates. People are normally quite optimistic - they will ignore bad news and embrace the good. This is what happened when the firefighters were relaxed; but when they were under stress, a different pattern emerged. Under these conditions, they became hyper-vigilant to bad news, even when it had nothing to do with their job (such as learning that the likelihood of card fraud was higher than they'd thought), and altered their beliefs in response. In contrast, stress didn't change how they responded to good news (such as learning that the likelihood of card fraud was lower than they'd thought). Back in our lab, we observed the same pattern in students who were told they had to give a surprise public speech, which would be judged by a panel, recorded and posted online. Sure enough, their cortisol levels spiked, their heart rates went up and they suddenly became better at processing unrelated, yet alarming, information about rates of disease and violence. When we experience stressful events, a physiological change is triggered that causes us to take in warnings and focus on what might go wrong. Brain imaging reveals that this 'switch' is related to a sudden boost in a neural signal important for learning, specifically in response to unexpected warning signs, such as faces expressing fear. Such neural engineering could have helped prehistoric humans to survive. When our ancestors found themselves surrounded by hungry animals, they would have benefited from an increased ability to learn about hazards. In a safe environment, however, it would have been wasteful to be on high alert constantly. So, a neural switch that automatically increases or decreases our ability to process warnings in response to changes in our environment could have been useful. In fact, people with clinical depression and anxiety seem unable to switch away from a state in which they absorb all the negative messages around them. It is also important to realise that stress travels rapidly from one person to the next. If a co-worker is stressed, we are more likely to tense up and feel stressed ourselves. We don't even need to be in the same room with someone for their emotions to influence our behaviour. Studies show that if we observe positive feeds on social media, such as images of a pink sunset, we are more likely to post uplifting messages ourselves. If we observe negative posts, such as complaints about a long queue at the coffee shop, we will in turn create more negative posts. In some ways, many of us now live as if we are in danger, constantly ready to tackle demanding emails and text messages, and respond to news alerts and comments on social media. Repeatedly checking your phone, according to a survey conducted by the American Psychological Association, is related to stress. In other words, a pre-programmed physiological reaction, which evolution has equipped us with to help us avoid famished predators, is now being triggered by an online post. Social media posting, according to one study, raises your pulse, makes you sweat, and enlarges your pupils more than most daily activities. The fact that stress increases the likelihood that we will focus more on alarming messages, together with the fact that it spreads extremely rapidly, can create collective fear that is not always justified. After a stressful public event, such as a natural disaster or major financial crash, there is often a wave of alarming information in traditional and social media, which individuals become very aware of. But that has the effect of exaggerating existing danger. And so, a reliable pattern emerges - stress is triggered, spreading from one person to the next, which temporarily enhances the likelihood that people will take in negative reports, which increases stress further. As a result, trips are cancelled, even if the disaster took place across the globe; stocks are sold, even when holding on is the best thing to do. The good news, however, is that positive emotions, such as hope, are contagious too, and are powerful in inducing people to act to find solutions. Being aware of the close relationship between people's emotional state and how they process information can help us frame our messages more effectively and become conscientious agents of change.This finding also has direct implications for our everyday lives. First, it reminds us to be especially cautious when making important decisions under stress. While stress can heighten our sensitivity to potential threats, this heightened awareness is not always beneficial—it can lead us to over-focus on negative information, overestimate risks, and underestimate opportunities. For example, in financial investments or career choices, stress may push us to avoid reasonable risks or overreact to low-probability events.\n",
    "\n",
    "Second, it suggests that we can actively manage our emotional state to improve information processing. Short relaxation exercises, deep breathing, mindfulness meditation, or talking with friends can reduce stress levels and help the brain return to a more balanced mode, allowing for a more comprehensive evaluation of information. Conversely, when facing real danger or urgent threats, stress can act as a protective mechanism, enabling rapid recognition of hazards and quick action.\n",
    "\n",
    "Furthermore, this mechanism plays a significant role in group behavior. Not only are we affected by our own stress, but we are also highly influenced by the emotional state of others. This means that in organizations, public communication, or even family life, the stress levels of leaders or key influencers can quickly spread, amplifying panic or hope. Being aware of this, we can strategically manage how information is presented—for instance, providing clear, calm guidance during crises while emphasizing solutions rather than just warnings.\n",
    "\n",
    "Ultimately, this research highlights a fundamental truth: human neural mechanisms are highly adaptive, designed both to protect us from real dangers and, in modern society, sometimes to overreact to virtual threats. Understanding this system makes it clearer how emotions, stress, and information processing interact to influence decision-making, allowing us to manage both personal behavior and social dynamics more effectively. By consciously regulating our emotions and environment, we can reduce unnecessary fear while harnessing the beneficial aspects of stress to respond more effectively to genuine challenges.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a4ed3f-4739-4527-9b9f-fae8d8780f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "class Tokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus=corpus.lower().replace(\"\\n\", \" \").replace(\".\",\" \")\n",
    "        self.corpus=re.sub(r\"[^\\w\\s]\",\"\",self.corpus)\n",
    "        self.tokens=re.split(r\"\\s+\", self.corpus)+[\"<unk>\"]\n",
    "        count_token=Counter(self.tokens)\n",
    "        self.token_freqs=[(*t,i) for i,t in enumerate(sorted(count_token.items(), key=lambda x:x[1], reverse=True))]\n",
    "        self.token_to_idx, self.idx_to_token={}, {}\n",
    "        for t in self.token_freqs:\n",
    "            self.token_to_idx[t[0]]=t[2]\n",
    "            self.idx_to_token[t[2]]=t[0]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text=re.sub(r\"[^\\w\\s]\",\"\", text.lower().replace(\"\\n\", \" \").replace(\".\",\" \"))\n",
    "        tokens=re.split(r\"\\s+\", text)\n",
    "        result=[]\n",
    "        for token in tokens:\n",
    "            if token in self.tokens:\n",
    "                result.append(self.token_to_idx[token])\n",
    "            else:\n",
    "                result.append(self.token_to_idx[\"<unk>\"])\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            return self.idx_to_token[idx]\n",
    "        elif isinstance(idx, list):\n",
    "            return [self.idx_to_token[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd23ff5-8a5a-408e-b845-505c7fbe19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer(text)\n",
    "\n",
    "corpus_tensor=torch.tensor(t(t.corpus), dtype=torch.long)\n",
    "y_corpus_tensor=torch.cat([corpus_tensor[-1:], corpus_tensor[:-1]])\n",
    "\n",
    "corpus_tensor=corpus_tensor.reshape(127,-1)\n",
    "y_corpus_tensor=y_corpus_tensor.reshape(127,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e3455f-4343-4479-adaa-2112701d6995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_tensor shape: torch.Size([1266, 4])\n",
      "y_corpus_tensor shape: torch.Size([1266, 4])\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = t(t.corpus)  \n",
    "seq_len = 4 \n",
    "\n",
    "X_list, y_list = [], []\n",
    "\n",
    "for i in range(len(corpus_indices) - seq_len):\n",
    "    X_list.append(corpus_indices[i:i+seq_len])\n",
    "    y_list.append(corpus_indices[i+1:i+seq_len+1]) \n",
    "\n",
    "corpus_tensor = torch.tensor(X_list, dtype=torch.long)       # (num_samples, seq_len)\n",
    "y_corpus_tensor = torch.tensor(y_list, dtype=torch.long)     # (num_samples, seq_len)\n",
    "\n",
    "print(\"corpus_tensor shape:\", corpus_tensor.shape)\n",
    "print(\"y_corpus_tensor shape:\", y_corpus_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ebcf536-4e83-4c49-915d-c6ae802d90eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=SimpleRNN(len(t), 2*len(t), 4*len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2cd31c4-2502-44da-b7ea-24e6b91d2c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (embedding): Embedding(559, 1118)\n",
       "  (rnn): RNN(1118, 2236, batch_first=True)\n",
       "  (fc): Linear(in_features=2236, out_features=559, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10d42d20-2e74-4cf4-b5d0-32c3deacb73b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.3383, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8690, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5544, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4703, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6639, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1310, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8173, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6394, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5375, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4748, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4370, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4140, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3986, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3893, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3832, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3792, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3767, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3742, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3718, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3702, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3690, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3678, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3665, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3655, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3648, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3642, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3636, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3632, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3631, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3626, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3621, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3617, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3613, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3612, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3610, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3609, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3607, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3604, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(38):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred=net(corpus_tensor)\n",
    "    loss=criterion(y_pred.reshape(-1,len(t)), y_corpus_tensor.flatten())\n",
    "    loss.backward()\n",
    "    gradient_clipping(net, 1) #or you can simply apply it on net.rnn\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0574aa8c-e3cb-4c9d-b122-2d74b987b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "def predict(text):\n",
    "    inputs=torch.tensor(t(text)).reshape(1,-1)\n",
    "    return t.decode(torch.argmax(net(inputs),dim=2).flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a3f1e84-5845-4488-8ff3-c94663d98565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mean\n",
      "I mean neuroscientists\n",
      "I mean neuroscientists wanted\n",
      "I mean neuroscientists wanted to\n",
      "I mean neuroscientists wanted to investigate\n",
      "I mean neuroscientists wanted to investigate how\n",
      "I mean neuroscientists wanted to investigate how the\n",
      "I mean neuroscientists wanted to investigate how the mind\n",
      "I mean neuroscientists wanted to investigate how the mind operates\n",
      "I mean neuroscientists wanted to investigate how the mind operates under\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so we\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so we visited\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so we visited some\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so we visited some local\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so we visited some local fire\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so we visited some local fire stations\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so we visited some local fire stations firefighters\n",
      "I mean neuroscientists wanted to investigate how the mind operates under stress so we visited some local fire stations firefighters workdays\n"
     ]
    }
   ],
   "source": [
    "results=[\"I mean\"]\n",
    "final_seq_len=20\n",
    "for i in range(final_seq_len-1):\n",
    "    new_word=predict(\" \".join(results))[-1]\n",
    "    print(\" \".join(results))\n",
    "    results.append(new_word)\n",
    "print(\" \".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b3ee0-2c7d-4667-aa52-7aeb8fc04a34",
   "metadata": {},
   "source": [
    "### 2.3 Long Short-Term Memory(LSTM)\n",
    "\n",
    ">*You should never use an RNN these days. You should always use an LSTM.*\n",
    "> *                                                    Christopher Manning\n",
    "\n",
    "RNN dones't handle well long-term dependency. So LSTM(Long Short-Term Memory) was proposed. It introduces a memory cell and a candidate memory cell used to update the memory cell; and 3 gates: a forget gate, an input gate and an output gate, respectively for deciding to what extent to delete from, write to and read from the memory cell. \n",
    "\n",
    "First, we compute the 3 gates, which are contrained to 0 to 1 by sigmoid. In the extreme case, when gate values are close to zero, the gate effectively blocks information flow; when they are close to one, the gate allows information to pass through almost fully.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F_t&=\\sigma(X_{t}W_{xf} + H_{t-1}W_{hf} + b_f) \\\\\n",
    "I_t&=\\sigma(X_{t}W_{xi} + H_{t-1}W_{hi} + b_i) \\\\\n",
    "O_t&=\\sigma(X_{t}W_{xo} + H_{t-1}W_{ho} + b_o)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then compute the candidate memoery cell.\n",
    "\n",
    "$$\n",
    "\\tilde{C_t}=\\tanh(X_{t}W_{xc} + H_{t-1}W_{hc} + b_c)\n",
    "$$\n",
    "\n",
    "Apply the forget gate to the previous memory cell, and apply the input gate to the current candidate cell to get the current memory cell.\n",
    "\n",
    "$$\n",
    "C_{t}=F_t \\odot C_{t-1} + I_t \\odot \\tilde{C_t}\n",
    "$$\n",
    "\n",
    "Apply the output gate to read from the memory cell, and get the hidden state.\n",
    "\n",
    "$$\n",
    "H_{t}=O_t \\odot tanh(C_{t})\n",
    "$$\n",
    "\n",
    "Finally we can connect a classification head to the $H_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d46dd4-acf2-47d2-b3e1-b8d37b2933cc",
   "metadata": {},
   "source": [
    "### 2.4 GRU(Gated Recurrent Unit)\n",
    "GRU is considered to be simpler than LSTM and often achieves comparable performance.\n",
    "\n",
    "An GRU has 2 gates: a reset gate and a update gate.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R_t&=\\sigma(X_{t}W_{xr} + H_{t-1}W_{hr} + b_r) \\\\\n",
    "U_t&=\\sigma(X_{t}W_{xu} + H_{t-1}W_{hu} + b_u) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then we use the reset gate to control how much past information is used to build a candidate hidden state.\n",
    "\n",
    "$$\n",
    "\\tilde{H_t}=tanh(X_{t}W_{xh} + (R_t \\odot H_{t-1})W_{hh} + b_h)\n",
    "$$\n",
    "\n",
    "Next, the update gate determines to what extent we update the hidden state.\n",
    "\n",
    "$$\n",
    "H_t=U_t \\odot H_{t-1} + (1-U_t) \\odot \\tilde{H_t}\n",
    "$$\n",
    "\n",
    "The update formula for $H_t$ is in the form of Exponential Moving Average(EMA). The definition of EMA is:\n",
    "\n",
    "$$\n",
    "EMA_{t} = \\alpha_{t} x_t + (1-\\alpha_{t}) EMA_{t-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168aea2e-37db-48d3-8c90-cd4a082dc42c",
   "metadata": {},
   "source": [
    "### 2.5 Why LSTM and GRU effectively alleviate gradient vanishing?\n",
    "\n",
    "The key lies in the \"+\" sign in $C_t$(LSTM) and $H_t$(GRU). Similar to Residual Connection (ResNet, Kaiming He), the \"+\" provides a term that has relatively stable gradients. In ResNet, the term is x, whose derivative is always 1. In LSTM, the term is $F_t \\odot C_{t-1}$, whose derivative $F_t$ is between 0 and 1, while in GRU it is $U_t \\odot H_{t-1}$, following the same principle. In contrast, vanilla RNNs (the most basic RNNs) don't have such a stable path for gradients to flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cab41-0afe-4074-87ca-5277e79b7e51",
   "metadata": {},
   "source": [
    "### Evaluation of Language Models\n",
    "#### Perplexity\n",
    "Given a sentence of n words, its perplexity is:\n",
    "$$\n",
    "\\begin{align}\n",
    "perplexity\n",
    "& = P(w_1, w_2, w_3, ..., w_n)^{- \\frac{1}{n}} \\\\\n",
    "& = (P(w_1) \\cdot P(w_2|w_1) \\cdot ... \\cdot P(w_n|w_{n-1}, w_{n-2},...,w_1))^{-\\frac{1}{n}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Usually we input a special token \"\\<BOS>\"(Beginning of Sentence) to get $P(w_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48478e5-302b-4bf2-9ff9-4d730b57017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SimpleRNN(100,200,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a41490c-9559-481e-9d16-0d180e0db9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    print(type(i))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
