{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dfa80e5-a889-4258-8bbd-65193c22b1a4",
   "metadata": {},
   "source": [
    "## How to represent meaning?\n",
    "\n",
    "Meaning is the combination of the signifier(symbol) and the signified(idea or thing). For example, the English word 'apple' acts as a signifier for the concept of a particular type of fruit. In Chinese, the signifier changes to '苹果' (píngguǒ), while the signified—the mental concept of that fruit—remains essentially the same.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dc5b13-e2cb-4d22-92cf-6568faa51f39",
   "metadata": {},
   "source": [
    "## How to organize words in computers?\n",
    "WordNet organize words by meaning. Specifically, it utilizes synonyms and antonyms, hypernyms and hyponyms, meronyms and holonyms.\n",
    "However, WordNet sometimes misclassifies words like good and proficient into synonyms, or misses new meaning of words like wicked, ninja, etc. Especially, it can't compute accurate word similarity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9b288-93ea-414b-a90b-ea06de5534d9",
   "metadata": {},
   "source": [
    "Traditional NLP represents words as one-hot vectors. The problem is that the vetor dimension depends on the size of the vocabulary. Say you have a dictionary as vocabulary, dimension disaster is bound to occur. And chances are that the number of dimensions is greater than the number of samples. Besides, one-hot word vectors and orthogonal, namely no similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501383c-be3b-4283-b73c-dda93210ec83",
   "metadata": {},
   "source": [
    "## Starting from Distributed Semantics: A word's meaning is given by the words that frequently appear close-by\n",
    "*\"You shall know a word by the company it keeps\"  --J.R. Firth*\n",
    "\n",
    "Based on this idea, people proposed Word2Vec (an implementation of word embedding that represents words using low-dimensional, dense real-valued vectors) to obtain word vectors with semantic features, which is somewhat analogous to a backbone network extracting features from images in computer vision. So unlike one-hot word vectors that are acquired through a tool class, Word2Vec word vectors are learnt.\n",
    "\n",
    "Word2Vec mainly has two implementations, forecasting context given centre(Skip-gram) and forecasting centre given context(CBOW). Take Skip-gram as an example, essentially, it is a multi-classification task. We want that, for every position *t*, for every word in the context with window step *j* and window size *m*, the product $P(w_{t+j} \\mid w_t)$ is maximal.\n",
    "\n",
    "$$\n",
    "\\prod_{t=1}^{T} \\prod_{\\substack{-m \\le j \\le m \\\\ j \\ne 0}} P(w_{t+j} \\mid w_t)\n",
    "$$\n",
    "\n",
    "\n",
    "For better optimization, we convert it to Negative Log Likelihood: $$\n",
    "-\\sum_{t=1}^{T} \\sum_{\\substack{-m \\le j \\le m \\\\ j \\ne 0}} \\log P(w_{t+j} \\mid w_t)\n",
    "$$\n",
    "\n",
    "This is actually Cross Entropy Loss, where $P(w_{t+j} \\mid w_t)$ is softmaxing model's output logits. So in reality, we use nn.CrossEntropyLoss() as the loss function. Specifically, we calculate $P(w_{t+j} \\mid w_t)$ this way:\n",
    "$$\n",
    "P(w_{t+j} \\mid w_t)=P(o \\mid c) = \\frac{\\exp \\big( v_c \\cdot u_o \\big)}{\\sum_{w=1}^V \\exp \\big( v_c \\cdot u_w \\big)}\n",
    "$$\n",
    ", where *v* is the word vector of centre words and *u* is the word vector of context words.\n",
    "### How to build the dataset?\n",
    "\n",
    "As shown in the formula $P(w_{t+j} \\mid w_t)$, every sample is composed of a centre word *$w_{t}$* and a context word *$w_{t+j}$*, so our features is the vectorization representation of *$w_{t}$*, and our label is that of *$w_{t+j}$*. If you use nn.Linear to implement the model, you should use one-hot encoding to vectorize words. For example, we have a corpus \"I love learning German and German history\". The vocabulary should be like \\[I, love, learning, German, and, history \\], which determines our one-hot vectors to be 6 dimensional(the size of the vocabulary). So word \"*I*\" is denoted as \\[1, 0, 0, 0, 0, 0\\] while word \"*German*\" is denoted as\\[0, 0, 0, 1, 0, 0\\]. However, due to the reason that nn.CrossEntropyLoss receives ordinal indices as label y, we need an different approach when handling the generation the y. The word \"*German*\", for instance, is encoded as *3*.\n",
    "\n",
    "### How to build the model architecture?\n",
    "\n",
    "Although we already have pytorch implementations like nn.Embedding, we can also implement this with nn.Linear. See details in the codes way below.\n",
    "\n",
    "```\n",
    "# Word Representation 结构图\n",
    "\n",
    "- **Word Vector（词向量，大类）**\n",
    "  - One-hot Vector（最原始的，高维、稀疏、无语义）\n",
    "  - **Word Embedding（更高级的，低维、稠密、有语义）**\n",
    "    - Word2Vec（Google 提出的具体算法，CBOW/Skip-gram）\n",
    "    - GloVe\n",
    "    - FastText\n",
    "    - （更复杂的神经网络模型）\n",
    "      - ELMo\n",
    "      - BERT\n",
    "      - GPT 系列\n",
    "\n",
    "---\n",
    "\n",
    "**Neural Word Representation（神经词表示）**  \n",
    ": 用神经网络学出来的词向量（涵盖 Word2Vec、GloVe、FastText、ELMo、BERT、GPT 等）\n",
    "```\n",
    "\n",
    "\n",
    "Token vs Type:\n",
    "\n",
    "\"Word word word word.\"\n",
    "\n",
    "​​Tokens​​ = 4  \n",
    "​​Types​​ = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dea4f4-dd74-46e4-9225-af2f3b0fb97f",
   "metadata": {},
   "source": [
    "## Below is an example of Skip-Gram(btw, skip-gram and CBOW are two implementations of Word2Vec. This note is all about Skip-Gram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "5f0048c9-2d43-47a5-b046-01b33bd1974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "\n",
    "corpus=\"\"\"Python is a popular programming language for machine learning and data science. \n",
    "Machine learning algorithms require large datasets for training neural networks. \n",
    "Deep learning models use multiple layers to extract features from raw data. \n",
    "Artificial intelligence is transforming various industries with automation solutions. \n",
    "Natural language processing helps computers understand human language patterns. \n",
    "Computer vision enables machines to recognize objects in images and videos. \n",
    "Data scientists use pandas numpy and matplotlib for data analysis tasks. \n",
    "Reinforcement learning agents learn through trial and error interactions. \n",
    "Cloud computing provides scalable resources for deploying AI applications. \n",
    "GitHub is a platform for version control and collaborative software development.\"\"\"\n",
    "\n",
    "tokens=corpus.lower().replace(\".\",'').replace(\"\\n\",\" \").split(\" \")\n",
    "\n",
    "counter=collections.Counter(tokens)\n",
    "\n",
    "vocab=[tp[0]for tp in sorted(counter.items(), key=lambda x:x[1], reverse=True)]\n",
    "vocab_size=len(vocab)\n",
    "\n",
    "word2idx={word:i for i,word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "a541ab82-4916-4290-8e87-adde44ee9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(word):\n",
    "    vector=torch.zeros(vocab_size)\n",
    "    vector[word2idx[word]]=1\n",
    "    return vector.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "a196bf5d-559e-47ea-bb60-8b8a1f7589c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(tokens, window=5):\n",
    "    X=None\n",
    "    y=None\n",
    "    for token in tokens:\n",
    "        for step in range(-window,window+1):\n",
    "            if step == 0:\n",
    "                continue\n",
    "            context_idx=tokens.index(token)+step\n",
    "            if context_idx < 0 or context_idx > len(tokens)-1:\n",
    "                continue\n",
    "            context=tokens[context_idx]\n",
    "            context_vocab_idx=vocab.index(context)\n",
    "            if X is None:\n",
    "                X=one_hot_encoder(token)\n",
    "            else:\n",
    "                X=torch.cat((X,one_hot_encoder(token)), dim=0)\n",
    "            if y is None:\n",
    "                y=torch.tensor(context_vocab_idx).reshape(1)\n",
    "            else:\n",
    "                y=torch.cat((y, torch.tensor(context_vocab_idx).reshape(1)), dim=0)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "748e74ca-511d-42e4-bd37-da05f260e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=generate_dataset(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "e0551128-e665-4275-ac66-c58c3137c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skip_Gram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=20):\n",
    "        super().__init__()\n",
    "        self.in_embed=nn.Linear(vocab_size, embedding_dim, bias=False) #its paramters are word vectors of centre words\n",
    "        self.out_embed=nn.Linear(embedding_dim, vocab_size, bias=False) #its paramters are word vectors of context words\n",
    "    def forward(self, x):\n",
    "        #事实上，用中心词向量和上下文词向量做点积计算条件概率的过程已经蕴含在这了\n",
    "        #In fact, the process of calculating conditional probabilities using dot product of center word vectors and context word vectors is already inherent in this\n",
    "        x=self.in_embed(x) #这一层的参数是所有中心词向量，用输入词的one-hot向量x 乘以中心词矩阵self.in_embed，相当于查表得到这个词的中心词向量\n",
    "        x=self.out_embed(x) #这一层的参数所有是上下文词向量，所以用上一层的输出点乘这一层，得到这个中心词与所有上下文词的余弦相似度。\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "c2461dd3-abe4-424d-a884-19ff9b52a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Skip_Gram(vocab_size)\n",
    "trainer=torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "f977d85f-4f99-4ea5-9228-0e47ccc764dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0707,  0.2475, -0.0775,  ..., -0.0339, -0.0314,  0.0535],\n",
       "        [-0.1219, -0.1411,  0.0738,  ...,  0.2032,  0.1827, -0.1409],\n",
       "        [-0.0892, -0.0587, -0.0309,  ...,  0.3755,  0.1125, -0.0848],\n",
       "        ...,\n",
       "        [ 0.2516,  0.0962,  0.0764,  ..., -0.0179, -0.0129,  0.0526],\n",
       "        [-0.0294, -0.2043,  0.1738,  ..., -0.0911, -0.1177, -0.1590],\n",
       "        [-0.2249,  0.1198, -0.1531,  ...,  0.2079, -0.1011,  0.1384]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_normal_(model.in_embed.weight)\n",
    "nn.init.xavier_normal_(model.out_embed.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "994ab284-3adf-4b9a-be29-074f128049c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "c496cdb2-03c1-42a3-8a09-b050ed5f5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2059, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2057, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2057, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2057, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2055, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2054, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2053, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2053, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    trainer.zero_grad()\n",
    "    y_hat=model(X)\n",
    "    l=loss(y_hat,y)\n",
    "    l.backward()\n",
    "    print(l)\n",
    "    trainer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "33d9eb2f-be5b-4ea4-a375-4207c9185d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.7468e-05, 7.7630e-05, 6.1342e-06, 1.8399e-06, 1.8599e-05, 2.0142e-01,\n",
       "        2.0714e-01, 1.8279e-01, 1.4354e-04, 2.6581e-11, 1.1548e-13, 3.0521e-04,\n",
       "        2.0582e-01, 2.0219e-01, 1.8282e-05, 1.6840e-09, 4.7620e-13, 5.4735e-14,\n",
       "        1.0344e-11, 3.4670e-12, 6.1300e-11, 5.9820e-13, 1.3153e-13, 3.8012e-14,\n",
       "        5.7839e-13, 8.4667e-13, 1.1247e-12, 2.6608e-12, 5.3314e-11, 1.2991e-13,\n",
       "        1.6635e-09, 6.0428e-12, 8.2682e-12, 5.6700e-12, 2.5304e-10, 1.4865e-10,\n",
       "        4.0416e-11, 2.7512e-09, 6.4789e-10, 3.1522e-09, 8.6565e-11, 2.1888e-11,\n",
       "        3.5582e-12, 7.0046e-14, 1.0264e-11, 1.5316e-14, 1.5014e-15, 8.0476e-15,\n",
       "        9.8369e-15, 5.7794e-14, 5.8810e-13, 6.1597e-14, 5.0919e-15, 2.1632e-12,\n",
       "        4.8041e-10, 2.4262e-10, 1.6591e-08, 5.8982e-10, 8.1182e-12, 7.3183e-11,\n",
       "        1.4661e-13, 1.0191e-10, 2.0629e-12, 1.3437e-13, 1.7435e-11, 3.1886e-12,\n",
       "        2.6553e-13, 1.5877e-13, 4.6620e-10, 1.1778e-09, 2.9650e-09, 6.0570e-09,\n",
       "        3.9406e-10, 2.0229e-09, 2.0453e-12, 2.8808e-08, 3.0822e-07, 2.6168e-13,\n",
       "        1.1875e-13, 2.2732e-11, 1.1458e-10, 9.8261e-10],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(model(X[0]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "1bc7b2fe-e584-45ed-8108-92a9dcc88b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1796, -1.2016,  2.1759, -0.2336,  1.0390,  1.0127,  0.5082,  1.5670,\n",
      "         -1.5129, -1.2642, -1.1036, -1.6216, -1.0297, -1.3005, -0.5369,  1.7246,\n",
      "          0.8961,  1.3457,  1.3210, -1.3263]]) tensor([[-2.2303,  0.1507,  1.3409, -1.0065,  2.3670,  1.2989,  1.0662,  0.4756,\n",
      "          0.0467, -0.7359, -0.6774, -1.5686, -0.5068, -1.0053,  0.0666,  0.9162,\n",
      "          0.7425,  1.7015,  1.4628, -0.2554]])\n",
      "相似度： 0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vv/v4gpq0_s6ybfp_z5170m7n6h0000gn/T/ipykernel_10168/1741129001.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vec1 = model.in_embed(torch.tensor(one_hot_encoder(word1))).detach()\n",
      "/var/folders/vv/v4gpq0_s6ybfp_z5170m7n6h0000gn/T/ipykernel_10168/1741129001.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vec2 = model.in_embed(torch.tensor(one_hot_encoder(word2))).detach()\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def word_similarity(word1, word2):\n",
    "    vec1 = model.in_embed(torch.tensor(one_hot_encoder(word1))).detach()\n",
    "    vec2 = model.in_embed(torch.tensor(one_hot_encoder(word2))).detach()\n",
    "    print(vec1,vec2)\n",
    "    return cosine_similarity(vec1, vec2, dim=1).item()\n",
    "\n",
    "print(f\"相似度： {word_similarity('machine', 'learning'):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
